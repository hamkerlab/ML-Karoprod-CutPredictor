{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cut Predictor # Tensorflow and optuna-based utility to learn to predict deviations from 1D position (or angles) based on a set of process parameters. Installation # Dependencies: numpy pandas matplotlib ipywidgets tensorflow >=2.6 optuna pip install git+https://github.com/hamkerlab/ML-Karoprod-CutPredictor.git@master","title":"Installation"},{"location":"#cut-predictor","text":"Tensorflow and optuna-based utility to learn to predict deviations from 1D position (or angles) based on a set of process parameters.","title":"Cut Predictor"},{"location":"#installation","text":"Dependencies: numpy pandas matplotlib ipywidgets tensorflow >=2.6 optuna pip install git+https://github.com/hamkerlab/ML-Karoprod-CutPredictor.git@master","title":"Installation"},{"location":"CutPredictor/","text":"CutPredictor # cut_predictor.CutPredictor # Bases: object Regression method to predict 1D cuts from process parameters. Source code in cut_predictor/Regressor.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 class CutPredictor ( object ): \"\"\" Regression method to predict 1D cuts from process parameters. \"\"\" def __init__ ( self ): # Empty model self . model = None # Not configured yet self . has_config = False self . data_loaded = False def load_data ( self , doe , data , process_parameters , position , output , categorical = [], angle = False , index = 'doe_id' ): \"\"\" Loads pandas Dataframes containing the data and preprocesses it. :param doe: pandas.Dataframe object containing the process parameters (design of experiments table). :param data: pandas.Dataframe object containing the experiments. :param process_parameters: list of process parameters ti be used. The names must match the columns of the csv file. :param categorical: list of process parameters that should be considered as categorical nad one-hot encoded. :param position: position variable. The name must match one column of the csv file. :param output: output variable(s) to be predicted. If several variables The name must match one column of the csv file. :param angle: if the position parameter is an angle, its sine and cosine are used as inputs instead. :param index: name of the column in doe and data representing the design ID (default: 'doe_id') \"\"\" self . has_config = True self . data_loaded = True # Attributes names self . process_parameters = process_parameters self . position_attribute = position if isinstance ( output , list ): self . output_attributes = output else : self . output_attributes = [ output ] self . categorical_attributes = categorical self . angle_input = angle self . doe_id = index self . features = [] self . categorical_values = {} # Min/Max/Mean/Std values self . min_values = {} self . max_values = {} self . mean_values = {} self . std_values = {} # Process parameters self . _preprocess_parameters ( doe ) # Expand the process parameters in the main df self . _preprocess_variables ( data ) # Get numpy arrays self . X = self . df [ self . features ] . to_numpy () self . target = self . df [ self . output_attributes ] . to_numpy () self . input_shape = ( self . X . shape [ 1 ], ) self . number_samples = self . X . shape [ 0 ] def _preprocess_parameters ( self , doe ): self . df_doe_raw = doe [[ self . doe_id ] + self . process_parameters ] self . df_doe = pd . DataFrame () self . df_doe [ self . doe_id ] = doe [ self . doe_id ] for attr in self . process_parameters : if not attr in self . categorical_attributes : # numerical data = doe [ attr ] self . features . append ( attr ) self . min_values [ attr ] = data . min () self . max_values [ attr ] = data . max () self . mean_values [ attr ] = data . mean () self . std_values [ attr ] = data . std () self . df_doe = self . df_doe . join (( data - self . mean_values [ attr ]) / self . std_values [ attr ]) else : # categorical self . categorical_values [ attr ] = sorted ( doe [ attr ] . unique ()) onehot = pd . get_dummies ( doe [ attr ], prefix = attr ) for val in onehot . keys (): self . features . append ( val ) self . df_doe = self . df_doe . join ( onehot ) def _preprocess_variables ( self , df ): # Position input and output variables for attr in [ self . position_attribute ] + self . output_attributes : data = df [ attr ] self . min_values [ attr ] = data . min () self . max_values [ attr ] = data . max () self . mean_values [ attr ] = data . mean () self . std_values [ attr ] = data . std () # Main dataframe self . df_raw = df [[ self . doe_id , self . position_attribute ] + self . output_attributes ] self . df = self . df_raw . merge ( self . df_doe , how = 'left' , on = self . doe_id ) self . df . drop ( self . doe_id , axis = 1 , inplace = True ) # Normalize input and outputs if not self . angle_input : self . df [ self . position_attribute ] = self . df [ self . position_attribute ] . apply ( lambda x : ( x - self . mean_values [ self . position_attribute ]) / ( self . std_values [ self . position_attribute ]) ) self . features . append ( self . position_attribute ) else : self . df [ \"cos_\" + self . position_attribute ] = np . cos ( self . df [ self . position_attribute ]) self . df [ \"sin_\" + self . position_attribute ] = np . sin ( self . df [ self . position_attribute ]) self . features . append ( \"cos_\" + self . position_attribute ) self . features . append ( \"sin_\" + self . position_attribute ) for attr in self . output_attributes : self . df [ attr ] = self . df [ attr ] . apply ( lambda x : ( x - self . min_values [ attr ]) / ( self . max_values [ attr ] - self . min_values [ attr ]) ) # Rescales the output def _rescale_output ( self , attr , y ): return self . min_values [ attr ] + ( self . max_values [ attr ] - self . min_values [ attr ]) * y def data_summary ( self ): \"\"\" Displays a summary of the loaded data. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return print ( \"Data summary \\n \" + \"-\" * 60 + \" \\n \" ) print ( \"Process parameters:\" ) for param in self . process_parameters : if param in self . categorical_attributes : print ( \" \\t -\" , param , \": categorical \" + str ( self . categorical_values [ param ]) ) else : print ( \" \\t -\" , param , \": numerical [\" , self . min_values [ param ], \" ... \" , self . max_values [ param ], \"]\" ) if self . angle_input : print ( \"Angle variable:\" ) else : print ( \"Position variable:\" ) print ( \" \\t -\" , self . position_attribute , \": numerical,\" , \"[\" , self . min_values [ self . position_attribute ], \"/\" , self . max_values [ self . position_attribute ], \"]\" ) print ( \"Output variable(s):\" ) for attr in self . output_attributes : print ( \" \\t -\" , attr , \": numerical,\" , \"[\" , self . min_values [ attr ], \"/\" , self . max_values [ attr ], \"]\" ) if self . data_loaded : print ( \" \\n Inputs \\n \" + \"-\" * 60 + \" \\n \" ) print ( self . X . shape ) print ( \" \\n Outputs \\n \" + \"-\" * 60 + \" \\n \" ) print ( self . target . shape ) def save_config ( self , filename ): \"\"\" Saves the configuration of the regressor, especially all variables derived from the data (min/max values, etc). Needed to make predictions from a trained model without having to reload the data. :param filename: path to the pickle file where the information will be saved. \"\"\" config = { # Features 'process_parameters' : self . process_parameters , 'position_attribute' : self . position_attribute , 'output_attributes' : self . output_attributes , 'categorical_attributes' : self . categorical_attributes , 'angle_input' : self . angle_input , 'doe_id' : self . doe_id , 'features' : self . features , 'categorical_values' : self . categorical_values , # Min/Max/Mean/Std values 'min_values' : self . min_values , 'max_values' : self . max_values , 'mean_values' : self . mean_values , 'std_values' : self . std_values , # Data shape 'input_shape' : self . input_shape , 'number_samples' : self . number_samples , } for key , val in config . items (): print ( key , val , type ( val )) with open ( filename , 'wb' ) as f : pickle . dump ( config , f , pickle . HIGHEST_PROTOCOL ) def load_config ( self , filename ): \"\"\" Loads data configuration from a pickle file created with save_config(). :param filename: path to the pickle file where the information was saved. \"\"\" with open ( filename , 'rb' ) as f : config = pickle . load ( f ) # Features self . process_parameters = config [ 'process_parameters' ] self . position_attribute = config [ 'position_attribute' ] self . output_attributes = config [ 'output_attributes' ] self . categorical_attributes = config [ 'categorical_attributes' ] self . angle_input = config [ 'angle_input' ] self . doe_id = config [ 'doe_id' ] self . features = config [ 'features' ] self . categorical_values = config [ 'categorical_values' ] # Min/Max/Mean/Std values self . min_values = config [ 'min_values' ] self . max_values = config [ 'max_values' ] self . mean_values = config [ 'mean_values' ] self . std_values = config [ 'std_values' ] # Data shape self . input_shape = config [ 'input_shape' ] self . number_samples = config [ 'number_samples' ] self . has_config = True def _create_model ( self , config ): # Clear the session tf . keras . backend . clear_session () # Create the model model = tf . keras . Sequential () model . add ( tf . keras . layers . Input ( self . input_shape )) # Add layers for n in config [ 'layers' ]: model . add ( tf . keras . layers . Dense ( n , activation = 'relu' )) if config [ 'dropout' ] > 0.0 : model . add ( tf . keras . layers . Dropout ( config [ 'dropout' ])) # Output layer model . add ( tf . keras . layers . Dense ( len ( self . output_attributes ))) # Compile model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = config [ 'learning_rate' ]), loss = tf . keras . losses . MeanSquaredError () ) return model def trial ( self , trial ): # Sample hyperparameters layers = [] nb_layers = trial . suggest_int ( 'nb_layers' , self . range_layers [ 0 ], self . range_layers [ 1 ]) for n in range ( nb_layers ): num_hidden = trial . suggest_int ( f 'n_units_l { n } ' , self . range_neurons [ 0 ], self . range_neurons [ 1 ], step = self . range_neurons [ 2 ]) layers . append ( num_hidden ) learning_rate = trial . suggest_loguniform ( 'learning_rate' , self . range_learning_rate [ 0 ], self . range_learning_rate [ 1 ]) dropout = trial . suggest_discrete_uniform ( 'dropout' , self . range_dropout [ 0 ], self . range_dropout [ 1 ], self . range_dropout [ 2 ]) config = { 'batch_size' : self . batch_size , 'max_epochs' : self . max_epochs , 'layers' : layers , 'dropout' : dropout , 'learning_rate' : learning_rate } # Create the model model = self . _create_model ( config ) # Train history = model . fit ( self . X , self . target , validation_split = 0.1 , epochs = self . max_epochs , batch_size = self . batch_size , verbose = 0 ) # Check performance val_mse = history . history [ 'val_loss' ][ - 1 ] # Save the best network if val_mse < self . best_mse : self . best_mse = val_mse model . save ( self . save_path ) self . best_history = history self . best_config = config return val_mse def autotune ( self , trials , save_path = 'best_model' , batch_size = 4096 , max_epochs = 20 , layers = [ 3 , 6 ], neurons = [ 64 , 512 , 32 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-6 , 1e-3 ] ): \"\"\" Searches for the optimal network configuration for the data. :param trials: number of trials to perform. :param save_path: path to save the best model (default: 'best_model'). :param batch_size: batch size to be used (default: 4096). :param max_epochs: maximum number of epochs for the training of a single network (default: 20) :param layers: range for the number of layers (default: [3, 6]). :param neurons: range (and optionally step) for the number of neurons per layer (default: [64, 512, 32]). If only two values are provided, the step is assumed to be 1. :param dropout: range and step for the dropout level (default: [0.0, 0.5, 0.1]). :param learning_rate: range for the learning rate (default: [1e-6, 1e-3]). The values will be sampled log-uniformly. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Save arguments self . save_path = save_path self . batch_size = batch_size self . max_epochs = max_epochs self . range_layers = layers self . range_neurons = neurons if len ( self . range_neurons ) == 2 : self . range_neurons . append ( 1 ) self . range_dropout = dropout self . range_learning_rate = learning_rate # Keep the best network only self . best_mse = 10000000.0 self . best_history = None # Start the study self . study = optuna . create_study ( direction = 'minimize' ) self . study . optimize ( self . trial , n_trials = trials ) if self . best_history is None : print ( \"Error: could not find a correct configuration\" ) return None # Reload the best model self . model = tf . keras . models . load_model ( self . save_path ) return self . best_config def custom_model ( self , save_path = 'best_model' , config = { 'batch_size' : 4096 , 'max_epochs' : 30 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.0 , 'learning_rate' : 0.005 }, verbose = False , ): \"\"\" Creates and trains a single model instead of the autotuning procedure. The dictionary describing the structure of the network must contain the following fields: * batch_size: batch size to be used (default: 4096). * max_epochs: maximum number of epochs for the training of a single network (default: 20) * layers: list of the number of neurons in each layer (default: [128, 128, 128, 128, 128]). * dropout: dropout level (default: 0.0). * learning_rate: learning rate (default: [0.005]). :param save_path: path to save the best model (default: 'best_model'). :param config: dictionary containing the description of the model. :param verbose: whether training details should be printed. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Save arguments self . save_path = save_path self . best_config = config self . batch_size = config [ 'batch_size' ] # Create the model self . model = self . _create_model ( self . best_config ) if verbose : self . model . summary () # Train history = self . model . fit ( self . X , self . target , validation_split = 0.1 , epochs = self . best_config [ 'max_epochs' ], batch_size = self . best_config [ 'batch_size' ], verbose = 1 if verbose else 0 ) # Check performance val_mse = history . history [ 'val_loss' ][ - 1 ] # Save the best network self . best_mse = val_mse self . model . save ( self . save_path ) self . best_history = history print ( \"Validation mse:\" , self . best_mse ) def training_summary ( self ): \"\"\" Creates various plots related to the best network. Can only be called after ``autotune()`` or ``custom_model``. You need to finally call `plt.show()` if you are in a script. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Training performance plt . figure () plt . plot ( self . best_history . history [ 'loss' ][:], label = \"training\" ) plt . plot ( self . best_history . history [ 'val_loss' ][:], label = \"validation\" ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"mse\" ) plt . title ( \"Training performance\" ) plt . legend () plt . savefig ( self . save_path + \"/training.png\" ) y = self . model . predict ( self . X , batch_size = self . batch_size ) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . scatter ( self . _rescale_output ( attr , self . target [:, idx ]), self . _rescale_output ( attr , y [:, idx ]), s = 1 ) plt . xlabel ( \"Ground truth\" ) plt . ylabel ( \"Prediction\" ) plt . title ( \"Ground truth vs. prediction for \" + attr ) plt . savefig ( self . save_path + \"/prediction_\" + attr + \".png\" ) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . subplot ( 121 ) plt . hist ( self . _rescale_output ( attr , self . target [:, idx ])) plt . xlabel ( \"Ground truth\" ) plt . ylabel ( attr ) plt . subplot ( 122 ) plt . hist ( self . _rescale_output ( attr , y [:, idx ])) plt . xlabel ( \"Prediction\" ) plt . savefig ( self . save_path + \"/distribution_\" + attr + \".png\" ) def load_network ( self , load_path = 'best_model' , batch_size = 4096 ): \"\"\" Load a pretrained network from a saved folder. The only parameter not saved by default is the batch size. :param load_path: path to the directory where the best network was saved (default: 'best_model') :param batch_size: batch size to be used (default: 4096). \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return self . batch_size = batch_size self . save_path = load_path self . model = tf . keras . models . load_model ( self . save_path ) def predict ( self , process_parameters , nb_points ): \"\"\" Predicts the output variable for a given number of input positions (uniformly distributed between the min/max values used for training). :param process_parameters: dictionary containing the value of all process parameters. :param nb_points: number of input positions to be used for the prediction. :return: (x, y) where x is a 1D position and y the value of each output attribute. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return if self . model is None : print ( \"Error: no model has been trained yet.\" ) return X = np . empty (( nb_points , 0 )) for idx , attr in enumerate ( self . process_parameters ): if attr in self . categorical_attributes : code = one_hot ([ process_parameters [ attr ]], self . categorical_values [ attr ]) code = np . repeat ( code , nb_points , axis = 0 ) X = np . concatenate (( X , code ), axis = 1 ) else : val = (( process_parameters [ attr ] - self . mean_values [ attr ] ) / self . std_values [ attr ]) * np . ones (( nb_points , 1 )) X = np . concatenate (( X , val ), axis = 1 ) # Position attribute is last position = np . linspace ( self . min_values [ self . position_attribute ], self . max_values [ self . position_attribute ], nb_points ) if not self . angle_input : values = ( position . reshape (( nb_points , 1 )) - self . mean_values [ self . position_attribute ] ) / self . std_values [ self . position_attribute ] X = np . concatenate (( X , values ), axis = 1 ) else : X = np . concatenate ( ( X , np . cos ( position ) . reshape (( nb_points , 1 )) ), axis = 1 ) X = np . concatenate ( ( X , np . sin ( position ) . reshape (( nb_points , 1 )) ), axis = 1 ) y = self . model . predict ( X , batch_size = self . batch_size ) . reshape (( nb_points , len ( self . output_attributes ))) for idx , attr in enumerate ( self . output_attributes ): y [:, idx ] = self . _rescale_output ( attr , y [:, idx ]) return position , y def compare ( self , doe_id ): \"\"\" Compares the prediction and the ground truth for the specified experiment. Creates a matplotlib figure. :param doe_id: id of the experiment. \"\"\" if self . model is None : print ( \"Error: no model has been trained yet.\" ) return indices = self . df_raw [ self . df_raw [ self . doe_id ] == doe_id ] . index . to_numpy () N = len ( indices ) X = self . X [ indices ] t = self . target [ indices ] for idx , attr in enumerate ( self . output_attributes ): t [:, idx ] = self . _rescale_output ( attr , t [:, idx ]) position = self . mean_values [ self . position_attribute ] + self . std_values [ self . position_attribute ] * X [:, - 1 ] # position is the last index y = self . model . predict ( X , batch_size = self . batch_size ) for idx , attr in enumerate ( self . output_attributes ): y [:, idx ] = self . _rescale_output ( attr , y [:, idx ]) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . plot ( position , y [:, idx ], label = \"prediction\" ) plt . plot ( position , t [:, idx ], label = \"data\" ) plt . xlabel ( self . position_attribute ) plt . ylabel ( attr ) plt . ylim (( self . min_values [ attr ], self . max_values [ attr ])) plt . legend () def interactive ( self ): \"\"\" Method to interactively vary the process parameters and predict the corresponding cut. Only work in a Jupyter notebook. ```python %matplotlib inline plt.rcParams['figure.dpi'] = 150 reg.interactive() ``` \"\"\" values = {} for attr in self . process_parameters : if attr in self . categorical_attributes : values [ attr ] = widgets . Dropdown ( options = self . categorical_values [ attr ], value = self . categorical_values [ attr ][ 0 ], ) else : values [ attr ] = widgets . FloatSlider ( value = self . mean_values [ attr ], min = self . min_values [ attr ], max = self . max_values [ attr ], step = ( self . max_values [ attr ] - self . min_values [ attr ]) / 100. , ) display ( widgets . interactive ( self . _visualize , ** values ) ) def _visualize ( self , ** values ): x , y = self . predict ( values , 100 ) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . plot ( x , y [:, idx ]) plt . xlabel ( self . position_attribute ) plt . ylabel ( attr ) plt . xlim (( self . min_values [ self . position_attribute ], self . max_values [ self . position_attribute ])) plt . ylim (( self . min_values [ attr ], self . max_values [ attr ])) plt . show () autotune ( trials , save_path = 'best_model' , batch_size = 4096 , max_epochs = 20 , layers = [ 3 , 6 ], neurons = [ 64 , 512 , 32 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-06 , 0.001 ]) # Searches for the optimal network configuration for the data. Parameters: Name Type Description Default trials number of trials to perform. required save_path path to save the best model (default: 'best_model'). 'best_model' batch_size batch size to be used (default: 4096). 4096 max_epochs maximum number of epochs for the training of a single network (default: 20) 20 layers range for the number of layers (default: [3, 6]). [3, 6] neurons range (and optionally step) for the number of neurons per layer (default: [64, 512, 32]). If only two values are provided, the step is assumed to be 1. [64, 512, 32] dropout range and step for the dropout level (default: [0.0, 0.5, 0.1]). [0.0, 0.5, 0.1] learning_rate range for the learning rate (default: [1e-6, 1e-3]). The values will be sampled log-uniformly. [1e-06, 0.001] Source code in cut_predictor/Regressor.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def autotune ( self , trials , save_path = 'best_model' , batch_size = 4096 , max_epochs = 20 , layers = [ 3 , 6 ], neurons = [ 64 , 512 , 32 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-6 , 1e-3 ] ): \"\"\" Searches for the optimal network configuration for the data. :param trials: number of trials to perform. :param save_path: path to save the best model (default: 'best_model'). :param batch_size: batch size to be used (default: 4096). :param max_epochs: maximum number of epochs for the training of a single network (default: 20) :param layers: range for the number of layers (default: [3, 6]). :param neurons: range (and optionally step) for the number of neurons per layer (default: [64, 512, 32]). If only two values are provided, the step is assumed to be 1. :param dropout: range and step for the dropout level (default: [0.0, 0.5, 0.1]). :param learning_rate: range for the learning rate (default: [1e-6, 1e-3]). The values will be sampled log-uniformly. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Save arguments self . save_path = save_path self . batch_size = batch_size self . max_epochs = max_epochs self . range_layers = layers self . range_neurons = neurons if len ( self . range_neurons ) == 2 : self . range_neurons . append ( 1 ) self . range_dropout = dropout self . range_learning_rate = learning_rate # Keep the best network only self . best_mse = 10000000.0 self . best_history = None # Start the study self . study = optuna . create_study ( direction = 'minimize' ) self . study . optimize ( self . trial , n_trials = trials ) if self . best_history is None : print ( \"Error: could not find a correct configuration\" ) return None # Reload the best model self . model = tf . keras . models . load_model ( self . save_path ) return self . best_config compare ( doe_id ) # Compares the prediction and the ground truth for the specified experiment. Creates a matplotlib figure. Parameters: Name Type Description Default doe_id id of the experiment. required Source code in cut_predictor/Regressor.py 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 def compare ( self , doe_id ): \"\"\" Compares the prediction and the ground truth for the specified experiment. Creates a matplotlib figure. :param doe_id: id of the experiment. \"\"\" if self . model is None : print ( \"Error: no model has been trained yet.\" ) return indices = self . df_raw [ self . df_raw [ self . doe_id ] == doe_id ] . index . to_numpy () N = len ( indices ) X = self . X [ indices ] t = self . target [ indices ] for idx , attr in enumerate ( self . output_attributes ): t [:, idx ] = self . _rescale_output ( attr , t [:, idx ]) position = self . mean_values [ self . position_attribute ] + self . std_values [ self . position_attribute ] * X [:, - 1 ] # position is the last index y = self . model . predict ( X , batch_size = self . batch_size ) for idx , attr in enumerate ( self . output_attributes ): y [:, idx ] = self . _rescale_output ( attr , y [:, idx ]) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . plot ( position , y [:, idx ], label = \"prediction\" ) plt . plot ( position , t [:, idx ], label = \"data\" ) plt . xlabel ( self . position_attribute ) plt . ylabel ( attr ) plt . ylim (( self . min_values [ attr ], self . max_values [ attr ])) plt . legend () custom_model ( save_path = 'best_model' , config = { 'batch_size' : 4096 , 'max_epochs' : 30 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.0 , 'learning_rate' : 0.005 }, verbose = False ) # Creates and trains a single model instead of the autotuning procedure. The dictionary describing the structure of the network must contain the following fields: batch_size: batch size to be used (default: 4096). max_epochs: maximum number of epochs for the training of a single network (default: 20) layers: list of the number of neurons in each layer (default: [128, 128, 128, 128, 128]). dropout: dropout level (default: 0.0). learning_rate: learning rate (default: [0.005]). Parameters: Name Type Description Default save_path path to save the best model (default: 'best_model'). 'best_model' config dictionary containing the description of the model. {'batch_size': 4096, 'max_epochs': 30, 'layers': [128, 128, 128, 128, 128], 'dropout': 0.0, 'learning_rate': 0.005} verbose whether training details should be printed. False Source code in cut_predictor/Regressor.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 def custom_model ( self , save_path = 'best_model' , config = { 'batch_size' : 4096 , 'max_epochs' : 30 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.0 , 'learning_rate' : 0.005 }, verbose = False , ): \"\"\" Creates and trains a single model instead of the autotuning procedure. The dictionary describing the structure of the network must contain the following fields: * batch_size: batch size to be used (default: 4096). * max_epochs: maximum number of epochs for the training of a single network (default: 20) * layers: list of the number of neurons in each layer (default: [128, 128, 128, 128, 128]). * dropout: dropout level (default: 0.0). * learning_rate: learning rate (default: [0.005]). :param save_path: path to save the best model (default: 'best_model'). :param config: dictionary containing the description of the model. :param verbose: whether training details should be printed. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Save arguments self . save_path = save_path self . best_config = config self . batch_size = config [ 'batch_size' ] # Create the model self . model = self . _create_model ( self . best_config ) if verbose : self . model . summary () # Train history = self . model . fit ( self . X , self . target , validation_split = 0.1 , epochs = self . best_config [ 'max_epochs' ], batch_size = self . best_config [ 'batch_size' ], verbose = 1 if verbose else 0 ) # Check performance val_mse = history . history [ 'val_loss' ][ - 1 ] # Save the best network self . best_mse = val_mse self . model . save ( self . save_path ) self . best_history = history print ( \"Validation mse:\" , self . best_mse ) data_summary () # Displays a summary of the loaded data. Source code in cut_predictor/Regressor.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def data_summary ( self ): \"\"\" Displays a summary of the loaded data. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return print ( \"Data summary \\n \" + \"-\" * 60 + \" \\n \" ) print ( \"Process parameters:\" ) for param in self . process_parameters : if param in self . categorical_attributes : print ( \" \\t -\" , param , \": categorical \" + str ( self . categorical_values [ param ]) ) else : print ( \" \\t -\" , param , \": numerical [\" , self . min_values [ param ], \" ... \" , self . max_values [ param ], \"]\" ) if self . angle_input : print ( \"Angle variable:\" ) else : print ( \"Position variable:\" ) print ( \" \\t -\" , self . position_attribute , \": numerical,\" , \"[\" , self . min_values [ self . position_attribute ], \"/\" , self . max_values [ self . position_attribute ], \"]\" ) print ( \"Output variable(s):\" ) for attr in self . output_attributes : print ( \" \\t -\" , attr , \": numerical,\" , \"[\" , self . min_values [ attr ], \"/\" , self . max_values [ attr ], \"]\" ) if self . data_loaded : print ( \" \\n Inputs \\n \" + \"-\" * 60 + \" \\n \" ) print ( self . X . shape ) print ( \" \\n Outputs \\n \" + \"-\" * 60 + \" \\n \" ) print ( self . target . shape ) interactive () # Method to interactively vary the process parameters and predict the corresponding cut. Only work in a Jupyter notebook. % matplotlib inline plt . rcParams [ 'figure.dpi' ] = 150 reg . interactive () Source code in cut_predictor/Regressor.py 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 def interactive ( self ): \"\"\" Method to interactively vary the process parameters and predict the corresponding cut. Only work in a Jupyter notebook. ```python %matplotlib inline plt.rcParams['figure.dpi'] = 150 reg.interactive() ``` \"\"\" values = {} for attr in self . process_parameters : if attr in self . categorical_attributes : values [ attr ] = widgets . Dropdown ( options = self . categorical_values [ attr ], value = self . categorical_values [ attr ][ 0 ], ) else : values [ attr ] = widgets . FloatSlider ( value = self . mean_values [ attr ], min = self . min_values [ attr ], max = self . max_values [ attr ], step = ( self . max_values [ attr ] - self . min_values [ attr ]) / 100. , ) display ( widgets . interactive ( self . _visualize , ** values ) ) load_config ( filename ) # Loads data configuration from a pickle file created with save_config(). Parameters: Name Type Description Default filename path to the pickle file where the information was saved. required Source code in cut_predictor/Regressor.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def load_config ( self , filename ): \"\"\" Loads data configuration from a pickle file created with save_config(). :param filename: path to the pickle file where the information was saved. \"\"\" with open ( filename , 'rb' ) as f : config = pickle . load ( f ) # Features self . process_parameters = config [ 'process_parameters' ] self . position_attribute = config [ 'position_attribute' ] self . output_attributes = config [ 'output_attributes' ] self . categorical_attributes = config [ 'categorical_attributes' ] self . angle_input = config [ 'angle_input' ] self . doe_id = config [ 'doe_id' ] self . features = config [ 'features' ] self . categorical_values = config [ 'categorical_values' ] # Min/Max/Mean/Std values self . min_values = config [ 'min_values' ] self . max_values = config [ 'max_values' ] self . mean_values = config [ 'mean_values' ] self . std_values = config [ 'std_values' ] # Data shape self . input_shape = config [ 'input_shape' ] self . number_samples = config [ 'number_samples' ] self . has_config = True load_data ( doe , data , process_parameters , position , output , categorical = [], angle = False , index = 'doe_id' ) # Loads pandas Dataframes containing the data and preprocesses it. Parameters: Name Type Description Default doe pandas.Dataframe object containing the process parameters (design of experiments table). required data pandas.Dataframe object containing the experiments. required process_parameters list of process parameters ti be used. The names must match the columns of the csv file. required categorical list of process parameters that should be considered as categorical nad one-hot encoded. [] position position variable. The name must match one column of the csv file. required output output variable(s) to be predicted. If several variables The name must match one column of the csv file. required angle if the position parameter is an angle, its sine and cosine are used as inputs instead. False index name of the column in doe and data representing the design ID (default: 'doe_id') 'doe_id' Source code in cut_predictor/Regressor.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def load_data ( self , doe , data , process_parameters , position , output , categorical = [], angle = False , index = 'doe_id' ): \"\"\" Loads pandas Dataframes containing the data and preprocesses it. :param doe: pandas.Dataframe object containing the process parameters (design of experiments table). :param data: pandas.Dataframe object containing the experiments. :param process_parameters: list of process parameters ti be used. The names must match the columns of the csv file. :param categorical: list of process parameters that should be considered as categorical nad one-hot encoded. :param position: position variable. The name must match one column of the csv file. :param output: output variable(s) to be predicted. If several variables The name must match one column of the csv file. :param angle: if the position parameter is an angle, its sine and cosine are used as inputs instead. :param index: name of the column in doe and data representing the design ID (default: 'doe_id') \"\"\" self . has_config = True self . data_loaded = True # Attributes names self . process_parameters = process_parameters self . position_attribute = position if isinstance ( output , list ): self . output_attributes = output else : self . output_attributes = [ output ] self . categorical_attributes = categorical self . angle_input = angle self . doe_id = index self . features = [] self . categorical_values = {} # Min/Max/Mean/Std values self . min_values = {} self . max_values = {} self . mean_values = {} self . std_values = {} # Process parameters self . _preprocess_parameters ( doe ) # Expand the process parameters in the main df self . _preprocess_variables ( data ) # Get numpy arrays self . X = self . df [ self . features ] . to_numpy () self . target = self . df [ self . output_attributes ] . to_numpy () self . input_shape = ( self . X . shape [ 1 ], ) self . number_samples = self . X . shape [ 0 ] load_network ( load_path = 'best_model' , batch_size = 4096 ) # Load a pretrained network from a saved folder. The only parameter not saved by default is the batch size. Parameters: Name Type Description Default load_path path to the directory where the best network was saved (default: 'best_model') 'best_model' batch_size batch size to be used (default: 4096). 4096 Source code in cut_predictor/Regressor.py 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 def load_network ( self , load_path = 'best_model' , batch_size = 4096 ): \"\"\" Load a pretrained network from a saved folder. The only parameter not saved by default is the batch size. :param load_path: path to the directory where the best network was saved (default: 'best_model') :param batch_size: batch size to be used (default: 4096). \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return self . batch_size = batch_size self . save_path = load_path self . model = tf . keras . models . load_model ( self . save_path ) predict ( process_parameters , nb_points ) # Predicts the output variable for a given number of input positions (uniformly distributed between the min/max values used for training). Parameters: Name Type Description Default process_parameters dictionary containing the value of all process parameters. required nb_points number of input positions to be used for the prediction. required Returns: Type Description (x, y) where x is a 1D position and y the value of each output attribute. Source code in cut_predictor/Regressor.py 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 def predict ( self , process_parameters , nb_points ): \"\"\" Predicts the output variable for a given number of input positions (uniformly distributed between the min/max values used for training). :param process_parameters: dictionary containing the value of all process parameters. :param nb_points: number of input positions to be used for the prediction. :return: (x, y) where x is a 1D position and y the value of each output attribute. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return if self . model is None : print ( \"Error: no model has been trained yet.\" ) return X = np . empty (( nb_points , 0 )) for idx , attr in enumerate ( self . process_parameters ): if attr in self . categorical_attributes : code = one_hot ([ process_parameters [ attr ]], self . categorical_values [ attr ]) code = np . repeat ( code , nb_points , axis = 0 ) X = np . concatenate (( X , code ), axis = 1 ) else : val = (( process_parameters [ attr ] - self . mean_values [ attr ] ) / self . std_values [ attr ]) * np . ones (( nb_points , 1 )) X = np . concatenate (( X , val ), axis = 1 ) # Position attribute is last position = np . linspace ( self . min_values [ self . position_attribute ], self . max_values [ self . position_attribute ], nb_points ) if not self . angle_input : values = ( position . reshape (( nb_points , 1 )) - self . mean_values [ self . position_attribute ] ) / self . std_values [ self . position_attribute ] X = np . concatenate (( X , values ), axis = 1 ) else : X = np . concatenate ( ( X , np . cos ( position ) . reshape (( nb_points , 1 )) ), axis = 1 ) X = np . concatenate ( ( X , np . sin ( position ) . reshape (( nb_points , 1 )) ), axis = 1 ) y = self . model . predict ( X , batch_size = self . batch_size ) . reshape (( nb_points , len ( self . output_attributes ))) for idx , attr in enumerate ( self . output_attributes ): y [:, idx ] = self . _rescale_output ( attr , y [:, idx ]) return position , y save_config ( filename ) # Saves the configuration of the regressor, especially all variables derived from the data (min/max values, etc). Needed to make predictions from a trained model without having to reload the data. Parameters: Name Type Description Default filename path to the pickle file where the information will be saved. required Source code in cut_predictor/Regressor.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def save_config ( self , filename ): \"\"\" Saves the configuration of the regressor, especially all variables derived from the data (min/max values, etc). Needed to make predictions from a trained model without having to reload the data. :param filename: path to the pickle file where the information will be saved. \"\"\" config = { # Features 'process_parameters' : self . process_parameters , 'position_attribute' : self . position_attribute , 'output_attributes' : self . output_attributes , 'categorical_attributes' : self . categorical_attributes , 'angle_input' : self . angle_input , 'doe_id' : self . doe_id , 'features' : self . features , 'categorical_values' : self . categorical_values , # Min/Max/Mean/Std values 'min_values' : self . min_values , 'max_values' : self . max_values , 'mean_values' : self . mean_values , 'std_values' : self . std_values , # Data shape 'input_shape' : self . input_shape , 'number_samples' : self . number_samples , } for key , val in config . items (): print ( key , val , type ( val )) with open ( filename , 'wb' ) as f : pickle . dump ( config , f , pickle . HIGHEST_PROTOCOL ) training_summary () # Creates various plots related to the best network. Can only be called after autotune() or custom_model . You need to finally call plt.show() if you are in a script. Source code in cut_predictor/Regressor.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def training_summary ( self ): \"\"\" Creates various plots related to the best network. Can only be called after ``autotune()`` or ``custom_model``. You need to finally call `plt.show()` if you are in a script. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Training performance plt . figure () plt . plot ( self . best_history . history [ 'loss' ][:], label = \"training\" ) plt . plot ( self . best_history . history [ 'val_loss' ][:], label = \"validation\" ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"mse\" ) plt . title ( \"Training performance\" ) plt . legend () plt . savefig ( self . save_path + \"/training.png\" ) y = self . model . predict ( self . X , batch_size = self . batch_size ) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . scatter ( self . _rescale_output ( attr , self . target [:, idx ]), self . _rescale_output ( attr , y [:, idx ]), s = 1 ) plt . xlabel ( \"Ground truth\" ) plt . ylabel ( \"Prediction\" ) plt . title ( \"Ground truth vs. prediction for \" + attr ) plt . savefig ( self . save_path + \"/prediction_\" + attr + \".png\" ) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . subplot ( 121 ) plt . hist ( self . _rescale_output ( attr , self . target [:, idx ])) plt . xlabel ( \"Ground truth\" ) plt . ylabel ( attr ) plt . subplot ( 122 ) plt . hist ( self . _rescale_output ( attr , y [:, idx ])) plt . xlabel ( \"Prediction\" ) plt . savefig ( self . save_path + \"/distribution_\" + attr + \".png\" )","title":"CutPredictor"},{"location":"CutPredictor/#cutpredictor","text":"","title":"CutPredictor"},{"location":"CutPredictor/#cut_predictor.CutPredictor","text":"Bases: object Regression method to predict 1D cuts from process parameters. Source code in cut_predictor/Regressor.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 class CutPredictor ( object ): \"\"\" Regression method to predict 1D cuts from process parameters. \"\"\" def __init__ ( self ): # Empty model self . model = None # Not configured yet self . has_config = False self . data_loaded = False def load_data ( self , doe , data , process_parameters , position , output , categorical = [], angle = False , index = 'doe_id' ): \"\"\" Loads pandas Dataframes containing the data and preprocesses it. :param doe: pandas.Dataframe object containing the process parameters (design of experiments table). :param data: pandas.Dataframe object containing the experiments. :param process_parameters: list of process parameters ti be used. The names must match the columns of the csv file. :param categorical: list of process parameters that should be considered as categorical nad one-hot encoded. :param position: position variable. The name must match one column of the csv file. :param output: output variable(s) to be predicted. If several variables The name must match one column of the csv file. :param angle: if the position parameter is an angle, its sine and cosine are used as inputs instead. :param index: name of the column in doe and data representing the design ID (default: 'doe_id') \"\"\" self . has_config = True self . data_loaded = True # Attributes names self . process_parameters = process_parameters self . position_attribute = position if isinstance ( output , list ): self . output_attributes = output else : self . output_attributes = [ output ] self . categorical_attributes = categorical self . angle_input = angle self . doe_id = index self . features = [] self . categorical_values = {} # Min/Max/Mean/Std values self . min_values = {} self . max_values = {} self . mean_values = {} self . std_values = {} # Process parameters self . _preprocess_parameters ( doe ) # Expand the process parameters in the main df self . _preprocess_variables ( data ) # Get numpy arrays self . X = self . df [ self . features ] . to_numpy () self . target = self . df [ self . output_attributes ] . to_numpy () self . input_shape = ( self . X . shape [ 1 ], ) self . number_samples = self . X . shape [ 0 ] def _preprocess_parameters ( self , doe ): self . df_doe_raw = doe [[ self . doe_id ] + self . process_parameters ] self . df_doe = pd . DataFrame () self . df_doe [ self . doe_id ] = doe [ self . doe_id ] for attr in self . process_parameters : if not attr in self . categorical_attributes : # numerical data = doe [ attr ] self . features . append ( attr ) self . min_values [ attr ] = data . min () self . max_values [ attr ] = data . max () self . mean_values [ attr ] = data . mean () self . std_values [ attr ] = data . std () self . df_doe = self . df_doe . join (( data - self . mean_values [ attr ]) / self . std_values [ attr ]) else : # categorical self . categorical_values [ attr ] = sorted ( doe [ attr ] . unique ()) onehot = pd . get_dummies ( doe [ attr ], prefix = attr ) for val in onehot . keys (): self . features . append ( val ) self . df_doe = self . df_doe . join ( onehot ) def _preprocess_variables ( self , df ): # Position input and output variables for attr in [ self . position_attribute ] + self . output_attributes : data = df [ attr ] self . min_values [ attr ] = data . min () self . max_values [ attr ] = data . max () self . mean_values [ attr ] = data . mean () self . std_values [ attr ] = data . std () # Main dataframe self . df_raw = df [[ self . doe_id , self . position_attribute ] + self . output_attributes ] self . df = self . df_raw . merge ( self . df_doe , how = 'left' , on = self . doe_id ) self . df . drop ( self . doe_id , axis = 1 , inplace = True ) # Normalize input and outputs if not self . angle_input : self . df [ self . position_attribute ] = self . df [ self . position_attribute ] . apply ( lambda x : ( x - self . mean_values [ self . position_attribute ]) / ( self . std_values [ self . position_attribute ]) ) self . features . append ( self . position_attribute ) else : self . df [ \"cos_\" + self . position_attribute ] = np . cos ( self . df [ self . position_attribute ]) self . df [ \"sin_\" + self . position_attribute ] = np . sin ( self . df [ self . position_attribute ]) self . features . append ( \"cos_\" + self . position_attribute ) self . features . append ( \"sin_\" + self . position_attribute ) for attr in self . output_attributes : self . df [ attr ] = self . df [ attr ] . apply ( lambda x : ( x - self . min_values [ attr ]) / ( self . max_values [ attr ] - self . min_values [ attr ]) ) # Rescales the output def _rescale_output ( self , attr , y ): return self . min_values [ attr ] + ( self . max_values [ attr ] - self . min_values [ attr ]) * y def data_summary ( self ): \"\"\" Displays a summary of the loaded data. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return print ( \"Data summary \\n \" + \"-\" * 60 + \" \\n \" ) print ( \"Process parameters:\" ) for param in self . process_parameters : if param in self . categorical_attributes : print ( \" \\t -\" , param , \": categorical \" + str ( self . categorical_values [ param ]) ) else : print ( \" \\t -\" , param , \": numerical [\" , self . min_values [ param ], \" ... \" , self . max_values [ param ], \"]\" ) if self . angle_input : print ( \"Angle variable:\" ) else : print ( \"Position variable:\" ) print ( \" \\t -\" , self . position_attribute , \": numerical,\" , \"[\" , self . min_values [ self . position_attribute ], \"/\" , self . max_values [ self . position_attribute ], \"]\" ) print ( \"Output variable(s):\" ) for attr in self . output_attributes : print ( \" \\t -\" , attr , \": numerical,\" , \"[\" , self . min_values [ attr ], \"/\" , self . max_values [ attr ], \"]\" ) if self . data_loaded : print ( \" \\n Inputs \\n \" + \"-\" * 60 + \" \\n \" ) print ( self . X . shape ) print ( \" \\n Outputs \\n \" + \"-\" * 60 + \" \\n \" ) print ( self . target . shape ) def save_config ( self , filename ): \"\"\" Saves the configuration of the regressor, especially all variables derived from the data (min/max values, etc). Needed to make predictions from a trained model without having to reload the data. :param filename: path to the pickle file where the information will be saved. \"\"\" config = { # Features 'process_parameters' : self . process_parameters , 'position_attribute' : self . position_attribute , 'output_attributes' : self . output_attributes , 'categorical_attributes' : self . categorical_attributes , 'angle_input' : self . angle_input , 'doe_id' : self . doe_id , 'features' : self . features , 'categorical_values' : self . categorical_values , # Min/Max/Mean/Std values 'min_values' : self . min_values , 'max_values' : self . max_values , 'mean_values' : self . mean_values , 'std_values' : self . std_values , # Data shape 'input_shape' : self . input_shape , 'number_samples' : self . number_samples , } for key , val in config . items (): print ( key , val , type ( val )) with open ( filename , 'wb' ) as f : pickle . dump ( config , f , pickle . HIGHEST_PROTOCOL ) def load_config ( self , filename ): \"\"\" Loads data configuration from a pickle file created with save_config(). :param filename: path to the pickle file where the information was saved. \"\"\" with open ( filename , 'rb' ) as f : config = pickle . load ( f ) # Features self . process_parameters = config [ 'process_parameters' ] self . position_attribute = config [ 'position_attribute' ] self . output_attributes = config [ 'output_attributes' ] self . categorical_attributes = config [ 'categorical_attributes' ] self . angle_input = config [ 'angle_input' ] self . doe_id = config [ 'doe_id' ] self . features = config [ 'features' ] self . categorical_values = config [ 'categorical_values' ] # Min/Max/Mean/Std values self . min_values = config [ 'min_values' ] self . max_values = config [ 'max_values' ] self . mean_values = config [ 'mean_values' ] self . std_values = config [ 'std_values' ] # Data shape self . input_shape = config [ 'input_shape' ] self . number_samples = config [ 'number_samples' ] self . has_config = True def _create_model ( self , config ): # Clear the session tf . keras . backend . clear_session () # Create the model model = tf . keras . Sequential () model . add ( tf . keras . layers . Input ( self . input_shape )) # Add layers for n in config [ 'layers' ]: model . add ( tf . keras . layers . Dense ( n , activation = 'relu' )) if config [ 'dropout' ] > 0.0 : model . add ( tf . keras . layers . Dropout ( config [ 'dropout' ])) # Output layer model . add ( tf . keras . layers . Dense ( len ( self . output_attributes ))) # Compile model . compile ( optimizer = tf . keras . optimizers . Adam ( learning_rate = config [ 'learning_rate' ]), loss = tf . keras . losses . MeanSquaredError () ) return model def trial ( self , trial ): # Sample hyperparameters layers = [] nb_layers = trial . suggest_int ( 'nb_layers' , self . range_layers [ 0 ], self . range_layers [ 1 ]) for n in range ( nb_layers ): num_hidden = trial . suggest_int ( f 'n_units_l { n } ' , self . range_neurons [ 0 ], self . range_neurons [ 1 ], step = self . range_neurons [ 2 ]) layers . append ( num_hidden ) learning_rate = trial . suggest_loguniform ( 'learning_rate' , self . range_learning_rate [ 0 ], self . range_learning_rate [ 1 ]) dropout = trial . suggest_discrete_uniform ( 'dropout' , self . range_dropout [ 0 ], self . range_dropout [ 1 ], self . range_dropout [ 2 ]) config = { 'batch_size' : self . batch_size , 'max_epochs' : self . max_epochs , 'layers' : layers , 'dropout' : dropout , 'learning_rate' : learning_rate } # Create the model model = self . _create_model ( config ) # Train history = model . fit ( self . X , self . target , validation_split = 0.1 , epochs = self . max_epochs , batch_size = self . batch_size , verbose = 0 ) # Check performance val_mse = history . history [ 'val_loss' ][ - 1 ] # Save the best network if val_mse < self . best_mse : self . best_mse = val_mse model . save ( self . save_path ) self . best_history = history self . best_config = config return val_mse def autotune ( self , trials , save_path = 'best_model' , batch_size = 4096 , max_epochs = 20 , layers = [ 3 , 6 ], neurons = [ 64 , 512 , 32 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-6 , 1e-3 ] ): \"\"\" Searches for the optimal network configuration for the data. :param trials: number of trials to perform. :param save_path: path to save the best model (default: 'best_model'). :param batch_size: batch size to be used (default: 4096). :param max_epochs: maximum number of epochs for the training of a single network (default: 20) :param layers: range for the number of layers (default: [3, 6]). :param neurons: range (and optionally step) for the number of neurons per layer (default: [64, 512, 32]). If only two values are provided, the step is assumed to be 1. :param dropout: range and step for the dropout level (default: [0.0, 0.5, 0.1]). :param learning_rate: range for the learning rate (default: [1e-6, 1e-3]). The values will be sampled log-uniformly. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Save arguments self . save_path = save_path self . batch_size = batch_size self . max_epochs = max_epochs self . range_layers = layers self . range_neurons = neurons if len ( self . range_neurons ) == 2 : self . range_neurons . append ( 1 ) self . range_dropout = dropout self . range_learning_rate = learning_rate # Keep the best network only self . best_mse = 10000000.0 self . best_history = None # Start the study self . study = optuna . create_study ( direction = 'minimize' ) self . study . optimize ( self . trial , n_trials = trials ) if self . best_history is None : print ( \"Error: could not find a correct configuration\" ) return None # Reload the best model self . model = tf . keras . models . load_model ( self . save_path ) return self . best_config def custom_model ( self , save_path = 'best_model' , config = { 'batch_size' : 4096 , 'max_epochs' : 30 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.0 , 'learning_rate' : 0.005 }, verbose = False , ): \"\"\" Creates and trains a single model instead of the autotuning procedure. The dictionary describing the structure of the network must contain the following fields: * batch_size: batch size to be used (default: 4096). * max_epochs: maximum number of epochs for the training of a single network (default: 20) * layers: list of the number of neurons in each layer (default: [128, 128, 128, 128, 128]). * dropout: dropout level (default: 0.0). * learning_rate: learning rate (default: [0.005]). :param save_path: path to save the best model (default: 'best_model'). :param config: dictionary containing the description of the model. :param verbose: whether training details should be printed. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Save arguments self . save_path = save_path self . best_config = config self . batch_size = config [ 'batch_size' ] # Create the model self . model = self . _create_model ( self . best_config ) if verbose : self . model . summary () # Train history = self . model . fit ( self . X , self . target , validation_split = 0.1 , epochs = self . best_config [ 'max_epochs' ], batch_size = self . best_config [ 'batch_size' ], verbose = 1 if verbose else 0 ) # Check performance val_mse = history . history [ 'val_loss' ][ - 1 ] # Save the best network self . best_mse = val_mse self . model . save ( self . save_path ) self . best_history = history print ( \"Validation mse:\" , self . best_mse ) def training_summary ( self ): \"\"\" Creates various plots related to the best network. Can only be called after ``autotune()`` or ``custom_model``. You need to finally call `plt.show()` if you are in a script. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Training performance plt . figure () plt . plot ( self . best_history . history [ 'loss' ][:], label = \"training\" ) plt . plot ( self . best_history . history [ 'val_loss' ][:], label = \"validation\" ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"mse\" ) plt . title ( \"Training performance\" ) plt . legend () plt . savefig ( self . save_path + \"/training.png\" ) y = self . model . predict ( self . X , batch_size = self . batch_size ) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . scatter ( self . _rescale_output ( attr , self . target [:, idx ]), self . _rescale_output ( attr , y [:, idx ]), s = 1 ) plt . xlabel ( \"Ground truth\" ) plt . ylabel ( \"Prediction\" ) plt . title ( \"Ground truth vs. prediction for \" + attr ) plt . savefig ( self . save_path + \"/prediction_\" + attr + \".png\" ) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . subplot ( 121 ) plt . hist ( self . _rescale_output ( attr , self . target [:, idx ])) plt . xlabel ( \"Ground truth\" ) plt . ylabel ( attr ) plt . subplot ( 122 ) plt . hist ( self . _rescale_output ( attr , y [:, idx ])) plt . xlabel ( \"Prediction\" ) plt . savefig ( self . save_path + \"/distribution_\" + attr + \".png\" ) def load_network ( self , load_path = 'best_model' , batch_size = 4096 ): \"\"\" Load a pretrained network from a saved folder. The only parameter not saved by default is the batch size. :param load_path: path to the directory where the best network was saved (default: 'best_model') :param batch_size: batch size to be used (default: 4096). \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return self . batch_size = batch_size self . save_path = load_path self . model = tf . keras . models . load_model ( self . save_path ) def predict ( self , process_parameters , nb_points ): \"\"\" Predicts the output variable for a given number of input positions (uniformly distributed between the min/max values used for training). :param process_parameters: dictionary containing the value of all process parameters. :param nb_points: number of input positions to be used for the prediction. :return: (x, y) where x is a 1D position and y the value of each output attribute. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return if self . model is None : print ( \"Error: no model has been trained yet.\" ) return X = np . empty (( nb_points , 0 )) for idx , attr in enumerate ( self . process_parameters ): if attr in self . categorical_attributes : code = one_hot ([ process_parameters [ attr ]], self . categorical_values [ attr ]) code = np . repeat ( code , nb_points , axis = 0 ) X = np . concatenate (( X , code ), axis = 1 ) else : val = (( process_parameters [ attr ] - self . mean_values [ attr ] ) / self . std_values [ attr ]) * np . ones (( nb_points , 1 )) X = np . concatenate (( X , val ), axis = 1 ) # Position attribute is last position = np . linspace ( self . min_values [ self . position_attribute ], self . max_values [ self . position_attribute ], nb_points ) if not self . angle_input : values = ( position . reshape (( nb_points , 1 )) - self . mean_values [ self . position_attribute ] ) / self . std_values [ self . position_attribute ] X = np . concatenate (( X , values ), axis = 1 ) else : X = np . concatenate ( ( X , np . cos ( position ) . reshape (( nb_points , 1 )) ), axis = 1 ) X = np . concatenate ( ( X , np . sin ( position ) . reshape (( nb_points , 1 )) ), axis = 1 ) y = self . model . predict ( X , batch_size = self . batch_size ) . reshape (( nb_points , len ( self . output_attributes ))) for idx , attr in enumerate ( self . output_attributes ): y [:, idx ] = self . _rescale_output ( attr , y [:, idx ]) return position , y def compare ( self , doe_id ): \"\"\" Compares the prediction and the ground truth for the specified experiment. Creates a matplotlib figure. :param doe_id: id of the experiment. \"\"\" if self . model is None : print ( \"Error: no model has been trained yet.\" ) return indices = self . df_raw [ self . df_raw [ self . doe_id ] == doe_id ] . index . to_numpy () N = len ( indices ) X = self . X [ indices ] t = self . target [ indices ] for idx , attr in enumerate ( self . output_attributes ): t [:, idx ] = self . _rescale_output ( attr , t [:, idx ]) position = self . mean_values [ self . position_attribute ] + self . std_values [ self . position_attribute ] * X [:, - 1 ] # position is the last index y = self . model . predict ( X , batch_size = self . batch_size ) for idx , attr in enumerate ( self . output_attributes ): y [:, idx ] = self . _rescale_output ( attr , y [:, idx ]) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . plot ( position , y [:, idx ], label = \"prediction\" ) plt . plot ( position , t [:, idx ], label = \"data\" ) plt . xlabel ( self . position_attribute ) plt . ylabel ( attr ) plt . ylim (( self . min_values [ attr ], self . max_values [ attr ])) plt . legend () def interactive ( self ): \"\"\" Method to interactively vary the process parameters and predict the corresponding cut. Only work in a Jupyter notebook. ```python %matplotlib inline plt.rcParams['figure.dpi'] = 150 reg.interactive() ``` \"\"\" values = {} for attr in self . process_parameters : if attr in self . categorical_attributes : values [ attr ] = widgets . Dropdown ( options = self . categorical_values [ attr ], value = self . categorical_values [ attr ][ 0 ], ) else : values [ attr ] = widgets . FloatSlider ( value = self . mean_values [ attr ], min = self . min_values [ attr ], max = self . max_values [ attr ], step = ( self . max_values [ attr ] - self . min_values [ attr ]) / 100. , ) display ( widgets . interactive ( self . _visualize , ** values ) ) def _visualize ( self , ** values ): x , y = self . predict ( values , 100 ) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . plot ( x , y [:, idx ]) plt . xlabel ( self . position_attribute ) plt . ylabel ( attr ) plt . xlim (( self . min_values [ self . position_attribute ], self . max_values [ self . position_attribute ])) plt . ylim (( self . min_values [ attr ], self . max_values [ attr ])) plt . show ()","title":"CutPredictor"},{"location":"CutPredictor/#cut_predictor.Regressor.CutPredictor.autotune","text":"Searches for the optimal network configuration for the data. Parameters: Name Type Description Default trials number of trials to perform. required save_path path to save the best model (default: 'best_model'). 'best_model' batch_size batch size to be used (default: 4096). 4096 max_epochs maximum number of epochs for the training of a single network (default: 20) 20 layers range for the number of layers (default: [3, 6]). [3, 6] neurons range (and optionally step) for the number of neurons per layer (default: [64, 512, 32]). If only two values are provided, the step is assumed to be 1. [64, 512, 32] dropout range and step for the dropout level (default: [0.0, 0.5, 0.1]). [0.0, 0.5, 0.1] learning_rate range for the learning rate (default: [1e-6, 1e-3]). The values will be sampled log-uniformly. [1e-06, 0.001] Source code in cut_predictor/Regressor.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 def autotune ( self , trials , save_path = 'best_model' , batch_size = 4096 , max_epochs = 20 , layers = [ 3 , 6 ], neurons = [ 64 , 512 , 32 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-6 , 1e-3 ] ): \"\"\" Searches for the optimal network configuration for the data. :param trials: number of trials to perform. :param save_path: path to save the best model (default: 'best_model'). :param batch_size: batch size to be used (default: 4096). :param max_epochs: maximum number of epochs for the training of a single network (default: 20) :param layers: range for the number of layers (default: [3, 6]). :param neurons: range (and optionally step) for the number of neurons per layer (default: [64, 512, 32]). If only two values are provided, the step is assumed to be 1. :param dropout: range and step for the dropout level (default: [0.0, 0.5, 0.1]). :param learning_rate: range for the learning rate (default: [1e-6, 1e-3]). The values will be sampled log-uniformly. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Save arguments self . save_path = save_path self . batch_size = batch_size self . max_epochs = max_epochs self . range_layers = layers self . range_neurons = neurons if len ( self . range_neurons ) == 2 : self . range_neurons . append ( 1 ) self . range_dropout = dropout self . range_learning_rate = learning_rate # Keep the best network only self . best_mse = 10000000.0 self . best_history = None # Start the study self . study = optuna . create_study ( direction = 'minimize' ) self . study . optimize ( self . trial , n_trials = trials ) if self . best_history is None : print ( \"Error: could not find a correct configuration\" ) return None # Reload the best model self . model = tf . keras . models . load_model ( self . save_path ) return self . best_config","title":"autotune()"},{"location":"CutPredictor/#cut_predictor.Regressor.CutPredictor.compare","text":"Compares the prediction and the ground truth for the specified experiment. Creates a matplotlib figure. Parameters: Name Type Description Default doe_id id of the experiment. required Source code in cut_predictor/Regressor.py 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 def compare ( self , doe_id ): \"\"\" Compares the prediction and the ground truth for the specified experiment. Creates a matplotlib figure. :param doe_id: id of the experiment. \"\"\" if self . model is None : print ( \"Error: no model has been trained yet.\" ) return indices = self . df_raw [ self . df_raw [ self . doe_id ] == doe_id ] . index . to_numpy () N = len ( indices ) X = self . X [ indices ] t = self . target [ indices ] for idx , attr in enumerate ( self . output_attributes ): t [:, idx ] = self . _rescale_output ( attr , t [:, idx ]) position = self . mean_values [ self . position_attribute ] + self . std_values [ self . position_attribute ] * X [:, - 1 ] # position is the last index y = self . model . predict ( X , batch_size = self . batch_size ) for idx , attr in enumerate ( self . output_attributes ): y [:, idx ] = self . _rescale_output ( attr , y [:, idx ]) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . plot ( position , y [:, idx ], label = \"prediction\" ) plt . plot ( position , t [:, idx ], label = \"data\" ) plt . xlabel ( self . position_attribute ) plt . ylabel ( attr ) plt . ylim (( self . min_values [ attr ], self . max_values [ attr ])) plt . legend ()","title":"compare()"},{"location":"CutPredictor/#cut_predictor.Regressor.CutPredictor.custom_model","text":"Creates and trains a single model instead of the autotuning procedure. The dictionary describing the structure of the network must contain the following fields: batch_size: batch size to be used (default: 4096). max_epochs: maximum number of epochs for the training of a single network (default: 20) layers: list of the number of neurons in each layer (default: [128, 128, 128, 128, 128]). dropout: dropout level (default: 0.0). learning_rate: learning rate (default: [0.005]). Parameters: Name Type Description Default save_path path to save the best model (default: 'best_model'). 'best_model' config dictionary containing the description of the model. {'batch_size': 4096, 'max_epochs': 30, 'layers': [128, 128, 128, 128, 128], 'dropout': 0.0, 'learning_rate': 0.005} verbose whether training details should be printed. False Source code in cut_predictor/Regressor.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 def custom_model ( self , save_path = 'best_model' , config = { 'batch_size' : 4096 , 'max_epochs' : 30 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.0 , 'learning_rate' : 0.005 }, verbose = False , ): \"\"\" Creates and trains a single model instead of the autotuning procedure. The dictionary describing the structure of the network must contain the following fields: * batch_size: batch size to be used (default: 4096). * max_epochs: maximum number of epochs for the training of a single network (default: 20) * layers: list of the number of neurons in each layer (default: [128, 128, 128, 128, 128]). * dropout: dropout level (default: 0.0). * learning_rate: learning rate (default: [0.005]). :param save_path: path to save the best model (default: 'best_model'). :param config: dictionary containing the description of the model. :param verbose: whether training details should be printed. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Save arguments self . save_path = save_path self . best_config = config self . batch_size = config [ 'batch_size' ] # Create the model self . model = self . _create_model ( self . best_config ) if verbose : self . model . summary () # Train history = self . model . fit ( self . X , self . target , validation_split = 0.1 , epochs = self . best_config [ 'max_epochs' ], batch_size = self . best_config [ 'batch_size' ], verbose = 1 if verbose else 0 ) # Check performance val_mse = history . history [ 'val_loss' ][ - 1 ] # Save the best network self . best_mse = val_mse self . model . save ( self . save_path ) self . best_history = history print ( \"Validation mse:\" , self . best_mse )","title":"custom_model()"},{"location":"CutPredictor/#cut_predictor.Regressor.CutPredictor.data_summary","text":"Displays a summary of the loaded data. Source code in cut_predictor/Regressor.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def data_summary ( self ): \"\"\" Displays a summary of the loaded data. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return print ( \"Data summary \\n \" + \"-\" * 60 + \" \\n \" ) print ( \"Process parameters:\" ) for param in self . process_parameters : if param in self . categorical_attributes : print ( \" \\t -\" , param , \": categorical \" + str ( self . categorical_values [ param ]) ) else : print ( \" \\t -\" , param , \": numerical [\" , self . min_values [ param ], \" ... \" , self . max_values [ param ], \"]\" ) if self . angle_input : print ( \"Angle variable:\" ) else : print ( \"Position variable:\" ) print ( \" \\t -\" , self . position_attribute , \": numerical,\" , \"[\" , self . min_values [ self . position_attribute ], \"/\" , self . max_values [ self . position_attribute ], \"]\" ) print ( \"Output variable(s):\" ) for attr in self . output_attributes : print ( \" \\t -\" , attr , \": numerical,\" , \"[\" , self . min_values [ attr ], \"/\" , self . max_values [ attr ], \"]\" ) if self . data_loaded : print ( \" \\n Inputs \\n \" + \"-\" * 60 + \" \\n \" ) print ( self . X . shape ) print ( \" \\n Outputs \\n \" + \"-\" * 60 + \" \\n \" ) print ( self . target . shape )","title":"data_summary()"},{"location":"CutPredictor/#cut_predictor.Regressor.CutPredictor.interactive","text":"Method to interactively vary the process parameters and predict the corresponding cut. Only work in a Jupyter notebook. % matplotlib inline plt . rcParams [ 'figure.dpi' ] = 150 reg . interactive () Source code in cut_predictor/Regressor.py 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 def interactive ( self ): \"\"\" Method to interactively vary the process parameters and predict the corresponding cut. Only work in a Jupyter notebook. ```python %matplotlib inline plt.rcParams['figure.dpi'] = 150 reg.interactive() ``` \"\"\" values = {} for attr in self . process_parameters : if attr in self . categorical_attributes : values [ attr ] = widgets . Dropdown ( options = self . categorical_values [ attr ], value = self . categorical_values [ attr ][ 0 ], ) else : values [ attr ] = widgets . FloatSlider ( value = self . mean_values [ attr ], min = self . min_values [ attr ], max = self . max_values [ attr ], step = ( self . max_values [ attr ] - self . min_values [ attr ]) / 100. , ) display ( widgets . interactive ( self . _visualize , ** values ) )","title":"interactive()"},{"location":"CutPredictor/#cut_predictor.Regressor.CutPredictor.load_config","text":"Loads data configuration from a pickle file created with save_config(). Parameters: Name Type Description Default filename path to the pickle file where the information was saved. required Source code in cut_predictor/Regressor.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def load_config ( self , filename ): \"\"\" Loads data configuration from a pickle file created with save_config(). :param filename: path to the pickle file where the information was saved. \"\"\" with open ( filename , 'rb' ) as f : config = pickle . load ( f ) # Features self . process_parameters = config [ 'process_parameters' ] self . position_attribute = config [ 'position_attribute' ] self . output_attributes = config [ 'output_attributes' ] self . categorical_attributes = config [ 'categorical_attributes' ] self . angle_input = config [ 'angle_input' ] self . doe_id = config [ 'doe_id' ] self . features = config [ 'features' ] self . categorical_values = config [ 'categorical_values' ] # Min/Max/Mean/Std values self . min_values = config [ 'min_values' ] self . max_values = config [ 'max_values' ] self . mean_values = config [ 'mean_values' ] self . std_values = config [ 'std_values' ] # Data shape self . input_shape = config [ 'input_shape' ] self . number_samples = config [ 'number_samples' ] self . has_config = True","title":"load_config()"},{"location":"CutPredictor/#cut_predictor.Regressor.CutPredictor.load_data","text":"Loads pandas Dataframes containing the data and preprocesses it. Parameters: Name Type Description Default doe pandas.Dataframe object containing the process parameters (design of experiments table). required data pandas.Dataframe object containing the experiments. required process_parameters list of process parameters ti be used. The names must match the columns of the csv file. required categorical list of process parameters that should be considered as categorical nad one-hot encoded. [] position position variable. The name must match one column of the csv file. required output output variable(s) to be predicted. If several variables The name must match one column of the csv file. required angle if the position parameter is an angle, its sine and cosine are used as inputs instead. False index name of the column in doe and data representing the design ID (default: 'doe_id') 'doe_id' Source code in cut_predictor/Regressor.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def load_data ( self , doe , data , process_parameters , position , output , categorical = [], angle = False , index = 'doe_id' ): \"\"\" Loads pandas Dataframes containing the data and preprocesses it. :param doe: pandas.Dataframe object containing the process parameters (design of experiments table). :param data: pandas.Dataframe object containing the experiments. :param process_parameters: list of process parameters ti be used. The names must match the columns of the csv file. :param categorical: list of process parameters that should be considered as categorical nad one-hot encoded. :param position: position variable. The name must match one column of the csv file. :param output: output variable(s) to be predicted. If several variables The name must match one column of the csv file. :param angle: if the position parameter is an angle, its sine and cosine are used as inputs instead. :param index: name of the column in doe and data representing the design ID (default: 'doe_id') \"\"\" self . has_config = True self . data_loaded = True # Attributes names self . process_parameters = process_parameters self . position_attribute = position if isinstance ( output , list ): self . output_attributes = output else : self . output_attributes = [ output ] self . categorical_attributes = categorical self . angle_input = angle self . doe_id = index self . features = [] self . categorical_values = {} # Min/Max/Mean/Std values self . min_values = {} self . max_values = {} self . mean_values = {} self . std_values = {} # Process parameters self . _preprocess_parameters ( doe ) # Expand the process parameters in the main df self . _preprocess_variables ( data ) # Get numpy arrays self . X = self . df [ self . features ] . to_numpy () self . target = self . df [ self . output_attributes ] . to_numpy () self . input_shape = ( self . X . shape [ 1 ], ) self . number_samples = self . X . shape [ 0 ]","title":"load_data()"},{"location":"CutPredictor/#cut_predictor.Regressor.CutPredictor.load_network","text":"Load a pretrained network from a saved folder. The only parameter not saved by default is the batch size. Parameters: Name Type Description Default load_path path to the directory where the best network was saved (default: 'best_model') 'best_model' batch_size batch size to be used (default: 4096). 4096 Source code in cut_predictor/Regressor.py 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 def load_network ( self , load_path = 'best_model' , batch_size = 4096 ): \"\"\" Load a pretrained network from a saved folder. The only parameter not saved by default is the batch size. :param load_path: path to the directory where the best network was saved (default: 'best_model') :param batch_size: batch size to be used (default: 4096). \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return self . batch_size = batch_size self . save_path = load_path self . model = tf . keras . models . load_model ( self . save_path )","title":"load_network()"},{"location":"CutPredictor/#cut_predictor.Regressor.CutPredictor.predict","text":"Predicts the output variable for a given number of input positions (uniformly distributed between the min/max values used for training). Parameters: Name Type Description Default process_parameters dictionary containing the value of all process parameters. required nb_points number of input positions to be used for the prediction. required Returns: Type Description (x, y) where x is a 1D position and y the value of each output attribute. Source code in cut_predictor/Regressor.py 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 def predict ( self , process_parameters , nb_points ): \"\"\" Predicts the output variable for a given number of input positions (uniformly distributed between the min/max values used for training). :param process_parameters: dictionary containing the value of all process parameters. :param nb_points: number of input positions to be used for the prediction. :return: (x, y) where x is a 1D position and y the value of each output attribute. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return if self . model is None : print ( \"Error: no model has been trained yet.\" ) return X = np . empty (( nb_points , 0 )) for idx , attr in enumerate ( self . process_parameters ): if attr in self . categorical_attributes : code = one_hot ([ process_parameters [ attr ]], self . categorical_values [ attr ]) code = np . repeat ( code , nb_points , axis = 0 ) X = np . concatenate (( X , code ), axis = 1 ) else : val = (( process_parameters [ attr ] - self . mean_values [ attr ] ) / self . std_values [ attr ]) * np . ones (( nb_points , 1 )) X = np . concatenate (( X , val ), axis = 1 ) # Position attribute is last position = np . linspace ( self . min_values [ self . position_attribute ], self . max_values [ self . position_attribute ], nb_points ) if not self . angle_input : values = ( position . reshape (( nb_points , 1 )) - self . mean_values [ self . position_attribute ] ) / self . std_values [ self . position_attribute ] X = np . concatenate (( X , values ), axis = 1 ) else : X = np . concatenate ( ( X , np . cos ( position ) . reshape (( nb_points , 1 )) ), axis = 1 ) X = np . concatenate ( ( X , np . sin ( position ) . reshape (( nb_points , 1 )) ), axis = 1 ) y = self . model . predict ( X , batch_size = self . batch_size ) . reshape (( nb_points , len ( self . output_attributes ))) for idx , attr in enumerate ( self . output_attributes ): y [:, idx ] = self . _rescale_output ( attr , y [:, idx ]) return position , y","title":"predict()"},{"location":"CutPredictor/#cut_predictor.Regressor.CutPredictor.save_config","text":"Saves the configuration of the regressor, especially all variables derived from the data (min/max values, etc). Needed to make predictions from a trained model without having to reload the data. Parameters: Name Type Description Default filename path to the pickle file where the information will be saved. required Source code in cut_predictor/Regressor.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 def save_config ( self , filename ): \"\"\" Saves the configuration of the regressor, especially all variables derived from the data (min/max values, etc). Needed to make predictions from a trained model without having to reload the data. :param filename: path to the pickle file where the information will be saved. \"\"\" config = { # Features 'process_parameters' : self . process_parameters , 'position_attribute' : self . position_attribute , 'output_attributes' : self . output_attributes , 'categorical_attributes' : self . categorical_attributes , 'angle_input' : self . angle_input , 'doe_id' : self . doe_id , 'features' : self . features , 'categorical_values' : self . categorical_values , # Min/Max/Mean/Std values 'min_values' : self . min_values , 'max_values' : self . max_values , 'mean_values' : self . mean_values , 'std_values' : self . std_values , # Data shape 'input_shape' : self . input_shape , 'number_samples' : self . number_samples , } for key , val in config . items (): print ( key , val , type ( val )) with open ( filename , 'wb' ) as f : pickle . dump ( config , f , pickle . HIGHEST_PROTOCOL )","title":"save_config()"},{"location":"CutPredictor/#cut_predictor.Regressor.CutPredictor.training_summary","text":"Creates various plots related to the best network. Can only be called after autotune() or custom_model . You need to finally call plt.show() if you are in a script. Source code in cut_predictor/Regressor.py 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 def training_summary ( self ): \"\"\" Creates various plots related to the best network. Can only be called after ``autotune()`` or ``custom_model``. You need to finally call `plt.show()` if you are in a script. \"\"\" if not self . has_config : print ( \"Error: The data has not been loaded yet.\" ) return # Training performance plt . figure () plt . plot ( self . best_history . history [ 'loss' ][:], label = \"training\" ) plt . plot ( self . best_history . history [ 'val_loss' ][:], label = \"validation\" ) plt . xlabel ( \"Epochs\" ) plt . ylabel ( \"mse\" ) plt . title ( \"Training performance\" ) plt . legend () plt . savefig ( self . save_path + \"/training.png\" ) y = self . model . predict ( self . X , batch_size = self . batch_size ) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . scatter ( self . _rescale_output ( attr , self . target [:, idx ]), self . _rescale_output ( attr , y [:, idx ]), s = 1 ) plt . xlabel ( \"Ground truth\" ) plt . ylabel ( \"Prediction\" ) plt . title ( \"Ground truth vs. prediction for \" + attr ) plt . savefig ( self . save_path + \"/prediction_\" + attr + \".png\" ) for idx , attr in enumerate ( self . output_attributes ): plt . figure () plt . subplot ( 121 ) plt . hist ( self . _rescale_output ( attr , self . target [:, idx ])) plt . xlabel ( \"Ground truth\" ) plt . ylabel ( attr ) plt . subplot ( 122 ) plt . hist ( self . _rescale_output ( attr , y [:, idx ])) plt . xlabel ( \"Prediction\" ) plt . savefig ( self . save_path + \"/distribution_\" + attr + \".png\" )","title":"training_summary()"},{"location":"Cut_flange/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Flange cut # import numpy as np import pandas as pd import matplotlib.pyplot as plt plt . rcParams [ 'figure.dpi' ] = 150 doe = pd . read_csv ( '../data/doe.csv' ) doe .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id Material_ID Blechdicke Niederhalterkraft Ziehspalt Stempel_ID Einlegeposition Ziehtiefe Breite UG OG E Material_Name Rp0 Rp0.2 Rp100 Rp25 Rp50 0 1 3 1.01 410 2.4 3 -5 30 70.2 1.71 2.00 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 1 2 5 1.48 400 2.4 3 -4 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 2 3 1 1.00 210 2.4 3 -1 70 70.2 1.71 2.00 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 3 4 2 1.19 390 2.4 3 5 30 70.2 1.71 2.00 200.03813 DC01_1.20mm 160.714935 174.535075 564.455438 424.931018 490.442223 4 5 1 1.00 360 1.6 2 3 30 71.8 1.14 1.33 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 995 996 3 1.01 90 1.6 2 0 70 71.8 1.14 1.33 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 996 997 4 0.99 480 2.4 3 5 30 70.2 1.71 2.00 71.94184 AA5083_1.00mm 248.796491 251.315902 385.373316 371.774337 384.299873 997 998 5 1.48 490 2.4 3 3 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 998 999 5 1.48 120 2.4 3 -3 30 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 999 1000 5 1.48 150 2.4 3 -2 70 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 1000 rows \u00d7 18 columns # Load the data using pandas data = pd . read_csv ( '../data/cut_flange_all.csv' ) data = data . head ( - 100 ) # remove last experiment data . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id c_phi c_rho c_z 0 1 1.221730 430.0 0.579342 1 1 1.225256 430.0 0.578100 2 1 1.228782 430.0 0.576825 3 1 1.232308 430.0 0.575515 4 1 1.235834 430.0 0.574172 5 1 1.239360 430.0 0.572797 6 1 1.242886 430.0 0.571389 7 1 1.246412 430.0 0.569949 8 1 1.249938 430.0 0.568479 9 1 1.253464 430.0 0.566978 from cut_predictor import CutPredictor reg = CutPredictor () reg . load_data ( doe = doe , data = data , index = 'doe_id' , process_parameters = [ 'Material_ID' , 'Niederhalterkraft' , 'Stempel_ID' , 'Einlegeposition' , 'Ziehtiefe' ], categorical = [ 'Material_ID' , 'Stempel_ID' , 'Einlegeposition' , 'Ziehtiefe' ], position = 'c_phi' , output = 'c_z' , angle = True , ) reg . data_summary () Data summary ------------------------------------------------------------ Process parameters: - Material_ID : categorical [1, 2, 3, 4, 5, 6] - Niederhalterkraft : numerical [ 10 ... 500 ] - Stempel_ID : categorical [2, 3] - Einlegeposition : categorical [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5] - Ziehtiefe : categorical [30, 50, 70] Angle variable: - c_phi : numerical, [ 1.2217304763960306 / 1.5707963267948966 ] Output variable(s): - c_z : numerical, [ 0.0401206149999993 / 3.6027990824451805 ] Inputs ------------------------------------------------------------ (88000, 25) Outputs ------------------------------------------------------------ (88000, 1) best_config = reg . autotune ( save_path = 'best_flange_model' , trials = 100 , max_epochs = 100 , layers = [ 3 , 5 ], neurons = [ 64 , 256 , 64 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-5 , 1e-3 ] ) print ( best_config ) config = { 'batch_size' : 4096 , 'max_epochs' : 100 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.0 , 'learning_rate' : 0.001 } # or best_config from autotune if you already did it once reg . custom_model ( save_path = 'best_flange_model' , config = config , verbose = True ) reg . training_summary () Metal device set to: Apple M1 Pro systemMemory: 16.00 GB maxCacheSize: 5.33 GB Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 128) 3328 dense_1 (Dense) (None, 128) 16512 dense_2 (Dense) (None, 128) 16512 dense_3 (Dense) (None, 128) 16512 dense_4 (Dense) (None, 128) 16512 dense_5 (Dense) (None, 1) 129 ================================================================= Total params: 69,505 Trainable params: 69,505 Non-trainable params: 0 _________________________________________________________________ Epoch 1/100 20/20 [==============================] - 1s 15ms/step - loss: 0.0338 - val_loss: 0.0098 Epoch 2/100 20/20 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0051 Epoch 3/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 0.0041 Epoch 4/100 20/20 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0037 Epoch 5/100 20/20 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0033 Epoch 6/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 0.0031 Epoch 7/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 0.0029 Epoch 8/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 0.0026 Epoch 9/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 0.0024 Epoch 10/100 20/20 [==============================] - 0s 12ms/step - loss: 8.9224e-04 - val_loss: 0.0021 Epoch 11/100 20/20 [==============================] - 0s 12ms/step - loss: 7.3106e-04 - val_loss: 0.0021 Epoch 12/100 20/20 [==============================] - 0s 11ms/step - loss: 6.0482e-04 - val_loss: 0.0021 Epoch 13/100 20/20 [==============================] - 0s 9ms/step - loss: 5.1156e-04 - val_loss: 0.0021 Epoch 14/100 20/20 [==============================] - 0s 10ms/step - loss: 4.3551e-04 - val_loss: 0.0021 Epoch 15/100 20/20 [==============================] - 0s 10ms/step - loss: 3.7039e-04 - val_loss: 0.0022 Epoch 16/100 20/20 [==============================] - 0s 9ms/step - loss: 3.5559e-04 - val_loss: 0.0022 Epoch 17/100 20/20 [==============================] - 0s 11ms/step - loss: 3.2070e-04 - val_loss: 0.0023 Epoch 18/100 20/20 [==============================] - 0s 13ms/step - loss: 2.5950e-04 - val_loss: 0.0025 Epoch 19/100 20/20 [==============================] - 0s 9ms/step - loss: 2.4061e-04 - val_loss: 0.0023 Epoch 20/100 20/20 [==============================] - 0s 11ms/step - loss: 2.1195e-04 - val_loss: 0.0024 Epoch 21/100 20/20 [==============================] - 0s 12ms/step - loss: 1.9945e-04 - val_loss: 0.0028 Epoch 22/100 20/20 [==============================] - 0s 11ms/step - loss: 2.3024e-04 - val_loss: 0.0026 Epoch 23/100 20/20 [==============================] - 0s 10ms/step - loss: 1.4745e-04 - val_loss: 0.0029 Epoch 24/100 20/20 [==============================] - 0s 10ms/step - loss: 1.4132e-04 - val_loss: 0.0029 Epoch 25/100 20/20 [==============================] - 0s 10ms/step - loss: 1.2949e-04 - val_loss: 0.0030 Epoch 26/100 20/20 [==============================] - 0s 9ms/step - loss: 1.2046e-04 - val_loss: 0.0032 Epoch 27/100 20/20 [==============================] - 0s 9ms/step - loss: 1.4146e-04 - val_loss: 0.0034 Epoch 28/100 20/20 [==============================] - 0s 11ms/step - loss: 1.4034e-04 - val_loss: 0.0031 Epoch 29/100 20/20 [==============================] - 0s 12ms/step - loss: 1.1022e-04 - val_loss: 0.0032 Epoch 30/100 20/20 [==============================] - 0s 9ms/step - loss: 8.5531e-05 - val_loss: 0.0033 Epoch 31/100 20/20 [==============================] - 0s 10ms/step - loss: 1.0001e-04 - val_loss: 0.0032 Epoch 32/100 20/20 [==============================] - 0s 9ms/step - loss: 8.7476e-05 - val_loss: 0.0034 Epoch 33/100 20/20 [==============================] - 0s 10ms/step - loss: 1.0243e-04 - val_loss: 0.0033 Epoch 34/100 20/20 [==============================] - 0s 12ms/step - loss: 7.5544e-05 - val_loss: 0.0033 Epoch 35/100 20/20 [==============================] - 0s 9ms/step - loss: 6.8444e-05 - val_loss: 0.0033 Epoch 36/100 20/20 [==============================] - 0s 11ms/step - loss: 1.1222e-04 - val_loss: 0.0032 Epoch 37/100 20/20 [==============================] - 0s 11ms/step - loss: 7.2705e-05 - val_loss: 0.0032 Epoch 38/100 20/20 [==============================] - 0s 12ms/step - loss: 6.4174e-05 - val_loss: 0.0033 Epoch 39/100 20/20 [==============================] - 0s 10ms/step - loss: 7.2747e-05 - val_loss: 0.0032 Epoch 40/100 20/20 [==============================] - 0s 11ms/step - loss: 8.1314e-05 - val_loss: 0.0032 Epoch 41/100 20/20 [==============================] - 0s 10ms/step - loss: 8.8074e-05 - val_loss: 0.0031 Epoch 42/100 20/20 [==============================] - 0s 11ms/step - loss: 6.2989e-05 - val_loss: 0.0032 Epoch 43/100 20/20 [==============================] - 0s 9ms/step - loss: 5.4174e-05 - val_loss: 0.0032 Epoch 44/100 20/20 [==============================] - 0s 9ms/step - loss: 5.9180e-05 - val_loss: 0.0033 Epoch 45/100 20/20 [==============================] - 0s 11ms/step - loss: 9.0560e-05 - val_loss: 0.0031 Epoch 46/100 20/20 [==============================] - 0s 11ms/step - loss: 4.8822e-05 - val_loss: 0.0032 Epoch 47/100 20/20 [==============================] - 0s 10ms/step - loss: 4.2065e-05 - val_loss: 0.0033 Epoch 48/100 20/20 [==============================] - 0s 11ms/step - loss: 9.9538e-05 - val_loss: 0.0031 Epoch 49/100 20/20 [==============================] - 0s 9ms/step - loss: 9.2275e-05 - val_loss: 0.0031 Epoch 50/100 20/20 [==============================] - 0s 9ms/step - loss: 4.8133e-05 - val_loss: 0.0030 Epoch 51/100 20/20 [==============================] - 0s 9ms/step - loss: 3.6958e-05 - val_loss: 0.0031 Epoch 52/100 20/20 [==============================] - 0s 9ms/step - loss: 4.4171e-05 - val_loss: 0.0031 Epoch 53/100 20/20 [==============================] - 0s 9ms/step - loss: 4.3603e-05 - val_loss: 0.0031 Epoch 54/100 20/20 [==============================] - 0s 9ms/step - loss: 1.2281e-04 - val_loss: 0.0030 Epoch 55/100 20/20 [==============================] - 0s 9ms/step - loss: 3.9202e-05 - val_loss: 0.0030 Epoch 56/100 20/20 [==============================] - 0s 9ms/step - loss: 3.1442e-05 - val_loss: 0.0031 Epoch 57/100 20/20 [==============================] - 0s 11ms/step - loss: 7.3913e-05 - val_loss: 0.0030 Epoch 58/100 20/20 [==============================] - 0s 11ms/step - loss: 4.4204e-05 - val_loss: 0.0030 Epoch 59/100 20/20 [==============================] - 0s 11ms/step - loss: 4.7389e-05 - val_loss: 0.0031 Epoch 60/100 20/20 [==============================] - 0s 9ms/step - loss: 7.3894e-05 - val_loss: 0.0030 Epoch 61/100 20/20 [==============================] - 0s 9ms/step - loss: 4.5373e-05 - val_loss: 0.0029 Epoch 62/100 20/20 [==============================] - 0s 9ms/step - loss: 3.6048e-05 - val_loss: 0.0030 Epoch 63/100 20/20 [==============================] - 0s 10ms/step - loss: 4.6843e-05 - val_loss: 0.0030 Epoch 64/100 20/20 [==============================] - 0s 9ms/step - loss: 6.6583e-05 - val_loss: 0.0031 Epoch 65/100 20/20 [==============================] - 0s 10ms/step - loss: 8.3047e-05 - val_loss: 0.0030 Epoch 66/100 20/20 [==============================] - 0s 11ms/step - loss: 3.1804e-05 - val_loss: 0.0029 Epoch 67/100 20/20 [==============================] - 0s 11ms/step - loss: 2.4485e-05 - val_loss: 0.0030 Epoch 68/100 20/20 [==============================] - 0s 11ms/step - loss: 3.4299e-05 - val_loss: 0.0030 Epoch 69/100 20/20 [==============================] - 0s 10ms/step - loss: 8.1273e-05 - val_loss: 0.0030 Epoch 70/100 20/20 [==============================] - 0s 11ms/step - loss: 6.3194e-05 - val_loss: 0.0029 Epoch 71/100 20/20 [==============================] - 0s 11ms/step - loss: 4.7397e-05 - val_loss: 0.0029 Epoch 72/100 20/20 [==============================] - 0s 9ms/step - loss: 3.0261e-05 - val_loss: 0.0029 Epoch 73/100 20/20 [==============================] - 0s 10ms/step - loss: 2.2114e-05 - val_loss: 0.0029 Epoch 74/100 20/20 [==============================] - 0s 11ms/step - loss: 8.4168e-05 - val_loss: 0.0030 Epoch 75/100 20/20 [==============================] - 0s 11ms/step - loss: 7.1212e-05 - val_loss: 0.0029 Epoch 76/100 20/20 [==============================] - 0s 11ms/step - loss: 3.3783e-05 - val_loss: 0.0028 Epoch 77/100 20/20 [==============================] - 0s 10ms/step - loss: 3.2825e-05 - val_loss: 0.0029 Epoch 78/100 20/20 [==============================] - 0s 11ms/step - loss: 3.5883e-05 - val_loss: 0.0028 Epoch 79/100 20/20 [==============================] - 0s 10ms/step - loss: 2.6065e-05 - val_loss: 0.0028 Epoch 80/100 20/20 [==============================] - 0s 11ms/step - loss: 4.5215e-05 - val_loss: 0.0029 Epoch 81/100 20/20 [==============================] - 0s 12ms/step - loss: 2.2126e-05 - val_loss: 0.0029 Epoch 82/100 20/20 [==============================] - 0s 11ms/step - loss: 6.5796e-05 - val_loss: 0.0028 Epoch 83/100 20/20 [==============================] - 0s 10ms/step - loss: 3.7644e-05 - val_loss: 0.0029 Epoch 84/100 20/20 [==============================] - 0s 10ms/step - loss: 3.8370e-05 - val_loss: 0.0029 Epoch 85/100 20/20 [==============================] - 0s 12ms/step - loss: 2.4181e-05 - val_loss: 0.0030 Epoch 86/100 20/20 [==============================] - 0s 11ms/step - loss: 7.5826e-05 - val_loss: 0.0030 Epoch 87/100 20/20 [==============================] - 0s 9ms/step - loss: 5.3050e-05 - val_loss: 0.0028 Epoch 88/100 20/20 [==============================] - 0s 11ms/step - loss: 3.1301e-05 - val_loss: 0.0028 Epoch 89/100 20/20 [==============================] - 0s 9ms/step - loss: 2.1576e-05 - val_loss: 0.0028 Epoch 90/100 20/20 [==============================] - 0s 10ms/step - loss: 2.9255e-05 - val_loss: 0.0028 Epoch 91/100 20/20 [==============================] - 0s 11ms/step - loss: 2.9431e-05 - val_loss: 0.0028 Epoch 92/100 20/20 [==============================] - 0s 9ms/step - loss: 6.9928e-05 - val_loss: 0.0028 Epoch 93/100 20/20 [==============================] - 0s 9ms/step - loss: 3.5810e-05 - val_loss: 0.0027 Epoch 94/100 20/20 [==============================] - 0s 9ms/step - loss: 1.8380e-05 - val_loss: 0.0028 Epoch 95/100 20/20 [==============================] - 0s 11ms/step - loss: 3.0884e-05 - val_loss: 0.0028 Epoch 96/100 20/20 [==============================] - 0s 11ms/step - loss: 5.8141e-05 - val_loss: 0.0028 Epoch 97/100 20/20 [==============================] - 0s 9ms/step - loss: 4.7468e-05 - val_loss: 0.0029 Epoch 98/100 20/20 [==============================] - 0s 11ms/step - loss: 2.7774e-05 - val_loss: 0.0028 Epoch 99/100 20/20 [==============================] - 0s 11ms/step - loss: 2.6202e-05 - val_loss: 0.0028 Epoch 100/100 20/20 [==============================] - 0s 9ms/step - loss: 5.1266e-05 - val_loss: 0.0028 INFO:tensorflow:Assets written to: best_flange_model/assets Validation mse: 0.0027520854491740465 idx = np . random . choice ( 878 ) + 1 print ( \"Doe_ID\" , idx ) reg . compare ( idx ) Doe_ID 767 % matplotlib inline plt . rcParams [ 'figure.dpi' ] = 150 reg . interactive () interactive(children=(Dropdown(description='Material_ID', options=(1, 2, 3, 4, 5, 6), value=1), FloatSlider(va\u2026","title":"Cut flange"},{"location":"Cut_flange/#flange-cut","text":"import numpy as np import pandas as pd import matplotlib.pyplot as plt plt . rcParams [ 'figure.dpi' ] = 150 doe = pd . read_csv ( '../data/doe.csv' ) doe .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id Material_ID Blechdicke Niederhalterkraft Ziehspalt Stempel_ID Einlegeposition Ziehtiefe Breite UG OG E Material_Name Rp0 Rp0.2 Rp100 Rp25 Rp50 0 1 3 1.01 410 2.4 3 -5 30 70.2 1.71 2.00 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 1 2 5 1.48 400 2.4 3 -4 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 2 3 1 1.00 210 2.4 3 -1 70 70.2 1.71 2.00 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 3 4 2 1.19 390 2.4 3 5 30 70.2 1.71 2.00 200.03813 DC01_1.20mm 160.714935 174.535075 564.455438 424.931018 490.442223 4 5 1 1.00 360 1.6 2 3 30 71.8 1.14 1.33 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 995 996 3 1.01 90 1.6 2 0 70 71.8 1.14 1.33 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 996 997 4 0.99 480 2.4 3 5 30 70.2 1.71 2.00 71.94184 AA5083_1.00mm 248.796491 251.315902 385.373316 371.774337 384.299873 997 998 5 1.48 490 2.4 3 3 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 998 999 5 1.48 120 2.4 3 -3 30 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 999 1000 5 1.48 150 2.4 3 -2 70 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 1000 rows \u00d7 18 columns # Load the data using pandas data = pd . read_csv ( '../data/cut_flange_all.csv' ) data = data . head ( - 100 ) # remove last experiment data . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id c_phi c_rho c_z 0 1 1.221730 430.0 0.579342 1 1 1.225256 430.0 0.578100 2 1 1.228782 430.0 0.576825 3 1 1.232308 430.0 0.575515 4 1 1.235834 430.0 0.574172 5 1 1.239360 430.0 0.572797 6 1 1.242886 430.0 0.571389 7 1 1.246412 430.0 0.569949 8 1 1.249938 430.0 0.568479 9 1 1.253464 430.0 0.566978 from cut_predictor import CutPredictor reg = CutPredictor () reg . load_data ( doe = doe , data = data , index = 'doe_id' , process_parameters = [ 'Material_ID' , 'Niederhalterkraft' , 'Stempel_ID' , 'Einlegeposition' , 'Ziehtiefe' ], categorical = [ 'Material_ID' , 'Stempel_ID' , 'Einlegeposition' , 'Ziehtiefe' ], position = 'c_phi' , output = 'c_z' , angle = True , ) reg . data_summary () Data summary ------------------------------------------------------------ Process parameters: - Material_ID : categorical [1, 2, 3, 4, 5, 6] - Niederhalterkraft : numerical [ 10 ... 500 ] - Stempel_ID : categorical [2, 3] - Einlegeposition : categorical [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5] - Ziehtiefe : categorical [30, 50, 70] Angle variable: - c_phi : numerical, [ 1.2217304763960306 / 1.5707963267948966 ] Output variable(s): - c_z : numerical, [ 0.0401206149999993 / 3.6027990824451805 ] Inputs ------------------------------------------------------------ (88000, 25) Outputs ------------------------------------------------------------ (88000, 1) best_config = reg . autotune ( save_path = 'best_flange_model' , trials = 100 , max_epochs = 100 , layers = [ 3 , 5 ], neurons = [ 64 , 256 , 64 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-5 , 1e-3 ] ) print ( best_config ) config = { 'batch_size' : 4096 , 'max_epochs' : 100 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.0 , 'learning_rate' : 0.001 } # or best_config from autotune if you already did it once reg . custom_model ( save_path = 'best_flange_model' , config = config , verbose = True ) reg . training_summary () Metal device set to: Apple M1 Pro systemMemory: 16.00 GB maxCacheSize: 5.33 GB Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 128) 3328 dense_1 (Dense) (None, 128) 16512 dense_2 (Dense) (None, 128) 16512 dense_3 (Dense) (None, 128) 16512 dense_4 (Dense) (None, 128) 16512 dense_5 (Dense) (None, 1) 129 ================================================================= Total params: 69,505 Trainable params: 69,505 Non-trainable params: 0 _________________________________________________________________ Epoch 1/100 20/20 [==============================] - 1s 15ms/step - loss: 0.0338 - val_loss: 0.0098 Epoch 2/100 20/20 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0051 Epoch 3/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 0.0041 Epoch 4/100 20/20 [==============================] - 0s 9ms/step - loss: 0.0026 - val_loss: 0.0037 Epoch 5/100 20/20 [==============================] - 0s 9ms/step - loss: 0.0022 - val_loss: 0.0033 Epoch 6/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 0.0031 Epoch 7/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 0.0029 Epoch 8/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 0.0026 Epoch 9/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 0.0024 Epoch 10/100 20/20 [==============================] - 0s 12ms/step - loss: 8.9224e-04 - val_loss: 0.0021 Epoch 11/100 20/20 [==============================] - 0s 12ms/step - loss: 7.3106e-04 - val_loss: 0.0021 Epoch 12/100 20/20 [==============================] - 0s 11ms/step - loss: 6.0482e-04 - val_loss: 0.0021 Epoch 13/100 20/20 [==============================] - 0s 9ms/step - loss: 5.1156e-04 - val_loss: 0.0021 Epoch 14/100 20/20 [==============================] - 0s 10ms/step - loss: 4.3551e-04 - val_loss: 0.0021 Epoch 15/100 20/20 [==============================] - 0s 10ms/step - loss: 3.7039e-04 - val_loss: 0.0022 Epoch 16/100 20/20 [==============================] - 0s 9ms/step - loss: 3.5559e-04 - val_loss: 0.0022 Epoch 17/100 20/20 [==============================] - 0s 11ms/step - loss: 3.2070e-04 - val_loss: 0.0023 Epoch 18/100 20/20 [==============================] - 0s 13ms/step - loss: 2.5950e-04 - val_loss: 0.0025 Epoch 19/100 20/20 [==============================] - 0s 9ms/step - loss: 2.4061e-04 - val_loss: 0.0023 Epoch 20/100 20/20 [==============================] - 0s 11ms/step - loss: 2.1195e-04 - val_loss: 0.0024 Epoch 21/100 20/20 [==============================] - 0s 12ms/step - loss: 1.9945e-04 - val_loss: 0.0028 Epoch 22/100 20/20 [==============================] - 0s 11ms/step - loss: 2.3024e-04 - val_loss: 0.0026 Epoch 23/100 20/20 [==============================] - 0s 10ms/step - loss: 1.4745e-04 - val_loss: 0.0029 Epoch 24/100 20/20 [==============================] - 0s 10ms/step - loss: 1.4132e-04 - val_loss: 0.0029 Epoch 25/100 20/20 [==============================] - 0s 10ms/step - loss: 1.2949e-04 - val_loss: 0.0030 Epoch 26/100 20/20 [==============================] - 0s 9ms/step - loss: 1.2046e-04 - val_loss: 0.0032 Epoch 27/100 20/20 [==============================] - 0s 9ms/step - loss: 1.4146e-04 - val_loss: 0.0034 Epoch 28/100 20/20 [==============================] - 0s 11ms/step - loss: 1.4034e-04 - val_loss: 0.0031 Epoch 29/100 20/20 [==============================] - 0s 12ms/step - loss: 1.1022e-04 - val_loss: 0.0032 Epoch 30/100 20/20 [==============================] - 0s 9ms/step - loss: 8.5531e-05 - val_loss: 0.0033 Epoch 31/100 20/20 [==============================] - 0s 10ms/step - loss: 1.0001e-04 - val_loss: 0.0032 Epoch 32/100 20/20 [==============================] - 0s 9ms/step - loss: 8.7476e-05 - val_loss: 0.0034 Epoch 33/100 20/20 [==============================] - 0s 10ms/step - loss: 1.0243e-04 - val_loss: 0.0033 Epoch 34/100 20/20 [==============================] - 0s 12ms/step - loss: 7.5544e-05 - val_loss: 0.0033 Epoch 35/100 20/20 [==============================] - 0s 9ms/step - loss: 6.8444e-05 - val_loss: 0.0033 Epoch 36/100 20/20 [==============================] - 0s 11ms/step - loss: 1.1222e-04 - val_loss: 0.0032 Epoch 37/100 20/20 [==============================] - 0s 11ms/step - loss: 7.2705e-05 - val_loss: 0.0032 Epoch 38/100 20/20 [==============================] - 0s 12ms/step - loss: 6.4174e-05 - val_loss: 0.0033 Epoch 39/100 20/20 [==============================] - 0s 10ms/step - loss: 7.2747e-05 - val_loss: 0.0032 Epoch 40/100 20/20 [==============================] - 0s 11ms/step - loss: 8.1314e-05 - val_loss: 0.0032 Epoch 41/100 20/20 [==============================] - 0s 10ms/step - loss: 8.8074e-05 - val_loss: 0.0031 Epoch 42/100 20/20 [==============================] - 0s 11ms/step - loss: 6.2989e-05 - val_loss: 0.0032 Epoch 43/100 20/20 [==============================] - 0s 9ms/step - loss: 5.4174e-05 - val_loss: 0.0032 Epoch 44/100 20/20 [==============================] - 0s 9ms/step - loss: 5.9180e-05 - val_loss: 0.0033 Epoch 45/100 20/20 [==============================] - 0s 11ms/step - loss: 9.0560e-05 - val_loss: 0.0031 Epoch 46/100 20/20 [==============================] - 0s 11ms/step - loss: 4.8822e-05 - val_loss: 0.0032 Epoch 47/100 20/20 [==============================] - 0s 10ms/step - loss: 4.2065e-05 - val_loss: 0.0033 Epoch 48/100 20/20 [==============================] - 0s 11ms/step - loss: 9.9538e-05 - val_loss: 0.0031 Epoch 49/100 20/20 [==============================] - 0s 9ms/step - loss: 9.2275e-05 - val_loss: 0.0031 Epoch 50/100 20/20 [==============================] - 0s 9ms/step - loss: 4.8133e-05 - val_loss: 0.0030 Epoch 51/100 20/20 [==============================] - 0s 9ms/step - loss: 3.6958e-05 - val_loss: 0.0031 Epoch 52/100 20/20 [==============================] - 0s 9ms/step - loss: 4.4171e-05 - val_loss: 0.0031 Epoch 53/100 20/20 [==============================] - 0s 9ms/step - loss: 4.3603e-05 - val_loss: 0.0031 Epoch 54/100 20/20 [==============================] - 0s 9ms/step - loss: 1.2281e-04 - val_loss: 0.0030 Epoch 55/100 20/20 [==============================] - 0s 9ms/step - loss: 3.9202e-05 - val_loss: 0.0030 Epoch 56/100 20/20 [==============================] - 0s 9ms/step - loss: 3.1442e-05 - val_loss: 0.0031 Epoch 57/100 20/20 [==============================] - 0s 11ms/step - loss: 7.3913e-05 - val_loss: 0.0030 Epoch 58/100 20/20 [==============================] - 0s 11ms/step - loss: 4.4204e-05 - val_loss: 0.0030 Epoch 59/100 20/20 [==============================] - 0s 11ms/step - loss: 4.7389e-05 - val_loss: 0.0031 Epoch 60/100 20/20 [==============================] - 0s 9ms/step - loss: 7.3894e-05 - val_loss: 0.0030 Epoch 61/100 20/20 [==============================] - 0s 9ms/step - loss: 4.5373e-05 - val_loss: 0.0029 Epoch 62/100 20/20 [==============================] - 0s 9ms/step - loss: 3.6048e-05 - val_loss: 0.0030 Epoch 63/100 20/20 [==============================] - 0s 10ms/step - loss: 4.6843e-05 - val_loss: 0.0030 Epoch 64/100 20/20 [==============================] - 0s 9ms/step - loss: 6.6583e-05 - val_loss: 0.0031 Epoch 65/100 20/20 [==============================] - 0s 10ms/step - loss: 8.3047e-05 - val_loss: 0.0030 Epoch 66/100 20/20 [==============================] - 0s 11ms/step - loss: 3.1804e-05 - val_loss: 0.0029 Epoch 67/100 20/20 [==============================] - 0s 11ms/step - loss: 2.4485e-05 - val_loss: 0.0030 Epoch 68/100 20/20 [==============================] - 0s 11ms/step - loss: 3.4299e-05 - val_loss: 0.0030 Epoch 69/100 20/20 [==============================] - 0s 10ms/step - loss: 8.1273e-05 - val_loss: 0.0030 Epoch 70/100 20/20 [==============================] - 0s 11ms/step - loss: 6.3194e-05 - val_loss: 0.0029 Epoch 71/100 20/20 [==============================] - 0s 11ms/step - loss: 4.7397e-05 - val_loss: 0.0029 Epoch 72/100 20/20 [==============================] - 0s 9ms/step - loss: 3.0261e-05 - val_loss: 0.0029 Epoch 73/100 20/20 [==============================] - 0s 10ms/step - loss: 2.2114e-05 - val_loss: 0.0029 Epoch 74/100 20/20 [==============================] - 0s 11ms/step - loss: 8.4168e-05 - val_loss: 0.0030 Epoch 75/100 20/20 [==============================] - 0s 11ms/step - loss: 7.1212e-05 - val_loss: 0.0029 Epoch 76/100 20/20 [==============================] - 0s 11ms/step - loss: 3.3783e-05 - val_loss: 0.0028 Epoch 77/100 20/20 [==============================] - 0s 10ms/step - loss: 3.2825e-05 - val_loss: 0.0029 Epoch 78/100 20/20 [==============================] - 0s 11ms/step - loss: 3.5883e-05 - val_loss: 0.0028 Epoch 79/100 20/20 [==============================] - 0s 10ms/step - loss: 2.6065e-05 - val_loss: 0.0028 Epoch 80/100 20/20 [==============================] - 0s 11ms/step - loss: 4.5215e-05 - val_loss: 0.0029 Epoch 81/100 20/20 [==============================] - 0s 12ms/step - loss: 2.2126e-05 - val_loss: 0.0029 Epoch 82/100 20/20 [==============================] - 0s 11ms/step - loss: 6.5796e-05 - val_loss: 0.0028 Epoch 83/100 20/20 [==============================] - 0s 10ms/step - loss: 3.7644e-05 - val_loss: 0.0029 Epoch 84/100 20/20 [==============================] - 0s 10ms/step - loss: 3.8370e-05 - val_loss: 0.0029 Epoch 85/100 20/20 [==============================] - 0s 12ms/step - loss: 2.4181e-05 - val_loss: 0.0030 Epoch 86/100 20/20 [==============================] - 0s 11ms/step - loss: 7.5826e-05 - val_loss: 0.0030 Epoch 87/100 20/20 [==============================] - 0s 9ms/step - loss: 5.3050e-05 - val_loss: 0.0028 Epoch 88/100 20/20 [==============================] - 0s 11ms/step - loss: 3.1301e-05 - val_loss: 0.0028 Epoch 89/100 20/20 [==============================] - 0s 9ms/step - loss: 2.1576e-05 - val_loss: 0.0028 Epoch 90/100 20/20 [==============================] - 0s 10ms/step - loss: 2.9255e-05 - val_loss: 0.0028 Epoch 91/100 20/20 [==============================] - 0s 11ms/step - loss: 2.9431e-05 - val_loss: 0.0028 Epoch 92/100 20/20 [==============================] - 0s 9ms/step - loss: 6.9928e-05 - val_loss: 0.0028 Epoch 93/100 20/20 [==============================] - 0s 9ms/step - loss: 3.5810e-05 - val_loss: 0.0027 Epoch 94/100 20/20 [==============================] - 0s 9ms/step - loss: 1.8380e-05 - val_loss: 0.0028 Epoch 95/100 20/20 [==============================] - 0s 11ms/step - loss: 3.0884e-05 - val_loss: 0.0028 Epoch 96/100 20/20 [==============================] - 0s 11ms/step - loss: 5.8141e-05 - val_loss: 0.0028 Epoch 97/100 20/20 [==============================] - 0s 9ms/step - loss: 4.7468e-05 - val_loss: 0.0029 Epoch 98/100 20/20 [==============================] - 0s 11ms/step - loss: 2.7774e-05 - val_loss: 0.0028 Epoch 99/100 20/20 [==============================] - 0s 11ms/step - loss: 2.6202e-05 - val_loss: 0.0028 Epoch 100/100 20/20 [==============================] - 0s 9ms/step - loss: 5.1266e-05 - val_loss: 0.0028 INFO:tensorflow:Assets written to: best_flange_model/assets Validation mse: 0.0027520854491740465 idx = np . random . choice ( 878 ) + 1 print ( \"Doe_ID\" , idx ) reg . compare ( idx ) Doe_ID 767 % matplotlib inline plt . rcParams [ 'figure.dpi' ] = 150 reg . interactive () interactive(children=(Dropdown(description='Material_ID', options=(1, 2, 3, 4, 5, 6), value=1), FloatSlider(va\u2026","title":"Flange cut"},{"location":"Cut_web/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Web cut # import numpy as np import pandas as pd import matplotlib.pyplot as plt plt . rcParams [ 'figure.dpi' ] = 150 doe = pd . read_csv ( '../data/doe.csv' ) doe .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id Material_ID Blechdicke Niederhalterkraft Ziehspalt Stempel_ID Einlegeposition Ziehtiefe Breite UG OG E Material_Name Rp0 Rp0.2 Rp100 Rp25 Rp50 0 1 3 1.01 410 2.4 3 -5 30 70.2 1.71 2.00 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 1 2 5 1.48 400 2.4 3 -4 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 2 3 1 1.00 210 2.4 3 -1 70 70.2 1.71 2.00 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 3 4 2 1.19 390 2.4 3 5 30 70.2 1.71 2.00 200.03813 DC01_1.20mm 160.714935 174.535075 564.455438 424.931018 490.442223 4 5 1 1.00 360 1.6 2 3 30 71.8 1.14 1.33 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 995 996 3 1.01 90 1.6 2 0 70 71.8 1.14 1.33 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 996 997 4 0.99 480 2.4 3 5 30 70.2 1.71 2.00 71.94184 AA5083_1.00mm 248.796491 251.315902 385.373316 371.774337 384.299873 997 998 5 1.48 490 2.4 3 3 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 998 999 5 1.48 120 2.4 3 -3 30 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 999 1000 5 1.48 150 2.4 3 -2 70 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 1000 rows \u00d7 18 columns data = pd . read_csv ( '../data/cut_web_all.csv' ) data = data . head ( - 100 ) # remove last experiment data . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id c_phi c_z c_rho 0 1 1.221730 25.0 463.681714 1 1 1.225256 25.0 463.680524 2 1 1.228782 25.0 463.678661 3 1 1.232308 25.0 463.708276 4 1 1.235834 25.0 463.736364 5 1 1.239360 25.0 463.753995 6 1 1.242886 25.0 463.795277 7 1 1.246412 25.0 463.832735 8 1 1.249938 25.0 463.881185 9 1 1.253464 25.0 463.918497 from cut_predictor import CutPredictor reg = CutPredictor () reg . load_data ( doe = doe , data = data , index = 'doe_id' , process_parameters = [ 'Material_ID' , 'Niederhalterkraft' , 'Stempel_ID' , 'Einlegeposition' , 'Ziehtiefe' ], categorical = [ 'Material_ID' , 'Stempel_ID' , 'Einlegeposition' , 'Ziehtiefe' ], position = 'c_phi' , output = 'c_rho' , angle = True , ) reg . data_summary () Data summary ------------------------------------------------------------ Process parameters: - Material_ID : categorical [1, 2, 3, 4, 5, 6] - Niederhalterkraft : numerical [ 10 ... 500 ] - Stempel_ID : categorical [2, 3] - Einlegeposition : categorical [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5] - Ziehtiefe : categorical [30, 50, 70] Angle variable: - c_phi : numerical, [ 1.2217304763960306 / 1.5707963267948966 ] Output variable(s): - c_rho : numerical, [ 462.8938277335998 / 465.1706438896841 ] Inputs ------------------------------------------------------------ (88000, 25) Outputs ------------------------------------------------------------ (88000, 1) best_config = reg . autotune ( save_path = 'best_web_model' , trials = 100 , max_epochs = 100 , layers = [ 2 , 4 ], neurons = [ 64 , 256 , 64 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-5 , 1e-3 ] ) print ( best_config ) config = { 'batch_size' : 4096 , 'max_epochs' : 100 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.1 , 'learning_rate' : 0.001 } # or best_config from autotune if you already did it once reg . custom_model ( save_path = 'best_web_model' , config = config , verbose = True ) reg . training_summary () Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 128) 3328 dropout (Dropout) (None, 128) 0 dense_1 (Dense) (None, 128) 16512 dropout_1 (Dropout) (None, 128) 0 dense_2 (Dense) (None, 128) 16512 dropout_2 (Dropout) (None, 128) 0 dense_3 (Dense) (None, 128) 16512 dropout_3 (Dropout) (None, 128) 0 dense_4 (Dense) (None, 128) 16512 dropout_4 (Dropout) (None, 128) 0 dense_5 (Dense) (None, 1) 129 ================================================================= Total params: 69,505 Trainable params: 69,505 Non-trainable params: 0 _________________________________________________________________ Epoch 1/100 20/20 [==============================] - 1s 16ms/step - loss: 0.0514 - val_loss: 0.0182 Epoch 2/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0227 - val_loss: 0.0220 Epoch 3/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0200 - val_loss: 0.0170 Epoch 4/100 20/20 [==============================] - 0s 14ms/step - loss: 0.0187 - val_loss: 0.0166 Epoch 5/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0177 - val_loss: 0.0171 Epoch 6/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0167 - val_loss: 0.0149 Epoch 7/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0159 - val_loss: 0.0146 Epoch 8/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0153 - val_loss: 0.0144 Epoch 9/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0146 - val_loss: 0.0134 Epoch 10/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0141 - val_loss: 0.0130 Epoch 11/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0136 - val_loss: 0.0127 Epoch 12/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0132 - val_loss: 0.0117 Epoch 13/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0128 - val_loss: 0.0113 Epoch 14/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0123 - val_loss: 0.0109 Epoch 15/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0118 - val_loss: 0.0095 Epoch 16/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0113 - val_loss: 0.0101 Epoch 17/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0109 - val_loss: 0.0080 Epoch 18/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0104 - val_loss: 0.0078 Epoch 19/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0100 - val_loss: 0.0079 Epoch 20/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0097 - val_loss: 0.0077 Epoch 21/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0093 - val_loss: 0.0067 Epoch 22/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0091 - val_loss: 0.0069 Epoch 23/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0089 - val_loss: 0.0070 Epoch 24/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0087 - val_loss: 0.0073 Epoch 25/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0085 - val_loss: 0.0065 Epoch 26/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0083 - val_loss: 0.0066 Epoch 27/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0083 - val_loss: 0.0070 Epoch 28/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0081 - val_loss: 0.0072 Epoch 29/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0080 - val_loss: 0.0072 Epoch 30/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0079 - val_loss: 0.0065 Epoch 31/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0078 - val_loss: 0.0065 Epoch 32/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0077 - val_loss: 0.0074 Epoch 33/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0077 - val_loss: 0.0058 Epoch 34/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0075 - val_loss: 0.0067 Epoch 35/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0074 - val_loss: 0.0068 Epoch 36/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0073 - val_loss: 0.0067 Epoch 37/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0073 - val_loss: 0.0059 Epoch 38/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0072 - val_loss: 0.0068 Epoch 39/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0072 - val_loss: 0.0065 Epoch 40/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0071 - val_loss: 0.0067 Epoch 41/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0070 - val_loss: 0.0058 Epoch 42/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0070 - val_loss: 0.0062 Epoch 43/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0069 - val_loss: 0.0060 Epoch 44/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0069 - val_loss: 0.0061 Epoch 45/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0069 - val_loss: 0.0068 Epoch 46/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0068 - val_loss: 0.0065 Epoch 47/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0067 - val_loss: 0.0061 Epoch 48/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0067 - val_loss: 0.0059 Epoch 49/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0066 - val_loss: 0.0060 Epoch 50/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0066 - val_loss: 0.0060 Epoch 51/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0066 - val_loss: 0.0060 Epoch 52/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0065 - val_loss: 0.0061 Epoch 53/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0065 - val_loss: 0.0061 Epoch 54/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0065 - val_loss: 0.0058 Epoch 55/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0064 - val_loss: 0.0056 Epoch 56/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0064 - val_loss: 0.0059 Epoch 57/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0064 - val_loss: 0.0061 Epoch 58/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0064 - val_loss: 0.0051 Epoch 59/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0064 - val_loss: 0.0058 Epoch 60/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0063 - val_loss: 0.0056 Epoch 61/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0063 - val_loss: 0.0057 Epoch 62/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0062 - val_loss: 0.0056 Epoch 63/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0062 - val_loss: 0.0053 Epoch 64/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0062 - val_loss: 0.0051 Epoch 65/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0062 - val_loss: 0.0056 Epoch 66/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0061 - val_loss: 0.0055 Epoch 67/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0061 - val_loss: 0.0055 Epoch 68/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0060 - val_loss: 0.0058 Epoch 69/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0060 - val_loss: 0.0058 Epoch 70/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0060 - val_loss: 0.0054 Epoch 71/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0060 - val_loss: 0.0049 Epoch 72/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0059 - val_loss: 0.0054 Epoch 73/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0059 - val_loss: 0.0052 Epoch 74/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0059 - val_loss: 0.0058 Epoch 75/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0059 - val_loss: 0.0056 Epoch 76/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0058 - val_loss: 0.0051 Epoch 77/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0058 - val_loss: 0.0054 Epoch 78/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0058 - val_loss: 0.0056 Epoch 79/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0058 - val_loss: 0.0051 Epoch 80/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0057 - val_loss: 0.0051 Epoch 81/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0057 - val_loss: 0.0055 Epoch 82/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0056 - val_loss: 0.0053 Epoch 83/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0057 - val_loss: 0.0055 Epoch 84/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0056 - val_loss: 0.0047 Epoch 85/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0056 - val_loss: 0.0049 Epoch 86/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0056 - val_loss: 0.0051 Epoch 87/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0055 - val_loss: 0.0051 Epoch 88/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0055 - val_loss: 0.0047 Epoch 89/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0054 - val_loss: 0.0054 Epoch 90/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0054 - val_loss: 0.0052 Epoch 91/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0054 - val_loss: 0.0053 Epoch 92/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0050 Epoch 93/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0053 - val_loss: 0.0050 Epoch 94/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0053 - val_loss: 0.0048 Epoch 95/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0052 - val_loss: 0.0047 Epoch 96/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0052 - val_loss: 0.0049 Epoch 97/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0051 - val_loss: 0.0048 Epoch 98/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0052 - val_loss: 0.0046 Epoch 99/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0051 - val_loss: 0.0043 Epoch 100/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0050 - val_loss: 0.0046 INFO:tensorflow:Assets written to: best_web_model/assets Validation mse: 0.004580171778798103 idx = np . random . choice ( 878 ) + 1 print ( \"Doe_ID\" , idx ) reg . compare ( idx ) Doe_ID 389 % matplotlib inline plt . rcParams [ 'figure.dpi' ] = 150 reg . interactive () interactive(children=(Dropdown(description='Material_ID', options=(1, 2, 3, 4, 5, 6), value=1), FloatSlider(va\u2026","title":"Cut web"},{"location":"Cut_web/#web-cut","text":"import numpy as np import pandas as pd import matplotlib.pyplot as plt plt . rcParams [ 'figure.dpi' ] = 150 doe = pd . read_csv ( '../data/doe.csv' ) doe .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id Material_ID Blechdicke Niederhalterkraft Ziehspalt Stempel_ID Einlegeposition Ziehtiefe Breite UG OG E Material_Name Rp0 Rp0.2 Rp100 Rp25 Rp50 0 1 3 1.01 410 2.4 3 -5 30 70.2 1.71 2.00 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 1 2 5 1.48 400 2.4 3 -4 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 2 3 1 1.00 210 2.4 3 -1 70 70.2 1.71 2.00 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 3 4 2 1.19 390 2.4 3 5 30 70.2 1.71 2.00 200.03813 DC01_1.20mm 160.714935 174.535075 564.455438 424.931018 490.442223 4 5 1 1.00 360 1.6 2 3 30 71.8 1.14 1.33 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 995 996 3 1.01 90 1.6 2 0 70 71.8 1.14 1.33 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 996 997 4 0.99 480 2.4 3 5 30 70.2 1.71 2.00 71.94184 AA5083_1.00mm 248.796491 251.315902 385.373316 371.774337 384.299873 997 998 5 1.48 490 2.4 3 3 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 998 999 5 1.48 120 2.4 3 -3 30 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 999 1000 5 1.48 150 2.4 3 -2 70 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 1000 rows \u00d7 18 columns data = pd . read_csv ( '../data/cut_web_all.csv' ) data = data . head ( - 100 ) # remove last experiment data . head ( 10 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id c_phi c_z c_rho 0 1 1.221730 25.0 463.681714 1 1 1.225256 25.0 463.680524 2 1 1.228782 25.0 463.678661 3 1 1.232308 25.0 463.708276 4 1 1.235834 25.0 463.736364 5 1 1.239360 25.0 463.753995 6 1 1.242886 25.0 463.795277 7 1 1.246412 25.0 463.832735 8 1 1.249938 25.0 463.881185 9 1 1.253464 25.0 463.918497 from cut_predictor import CutPredictor reg = CutPredictor () reg . load_data ( doe = doe , data = data , index = 'doe_id' , process_parameters = [ 'Material_ID' , 'Niederhalterkraft' , 'Stempel_ID' , 'Einlegeposition' , 'Ziehtiefe' ], categorical = [ 'Material_ID' , 'Stempel_ID' , 'Einlegeposition' , 'Ziehtiefe' ], position = 'c_phi' , output = 'c_rho' , angle = True , ) reg . data_summary () Data summary ------------------------------------------------------------ Process parameters: - Material_ID : categorical [1, 2, 3, 4, 5, 6] - Niederhalterkraft : numerical [ 10 ... 500 ] - Stempel_ID : categorical [2, 3] - Einlegeposition : categorical [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5] - Ziehtiefe : categorical [30, 50, 70] Angle variable: - c_phi : numerical, [ 1.2217304763960306 / 1.5707963267948966 ] Output variable(s): - c_rho : numerical, [ 462.8938277335998 / 465.1706438896841 ] Inputs ------------------------------------------------------------ (88000, 25) Outputs ------------------------------------------------------------ (88000, 1) best_config = reg . autotune ( save_path = 'best_web_model' , trials = 100 , max_epochs = 100 , layers = [ 2 , 4 ], neurons = [ 64 , 256 , 64 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-5 , 1e-3 ] ) print ( best_config ) config = { 'batch_size' : 4096 , 'max_epochs' : 100 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.1 , 'learning_rate' : 0.001 } # or best_config from autotune if you already did it once reg . custom_model ( save_path = 'best_web_model' , config = config , verbose = True ) reg . training_summary () Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 128) 3328 dropout (Dropout) (None, 128) 0 dense_1 (Dense) (None, 128) 16512 dropout_1 (Dropout) (None, 128) 0 dense_2 (Dense) (None, 128) 16512 dropout_2 (Dropout) (None, 128) 0 dense_3 (Dense) (None, 128) 16512 dropout_3 (Dropout) (None, 128) 0 dense_4 (Dense) (None, 128) 16512 dropout_4 (Dropout) (None, 128) 0 dense_5 (Dense) (None, 1) 129 ================================================================= Total params: 69,505 Trainable params: 69,505 Non-trainable params: 0 _________________________________________________________________ Epoch 1/100 20/20 [==============================] - 1s 16ms/step - loss: 0.0514 - val_loss: 0.0182 Epoch 2/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0227 - val_loss: 0.0220 Epoch 3/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0200 - val_loss: 0.0170 Epoch 4/100 20/20 [==============================] - 0s 14ms/step - loss: 0.0187 - val_loss: 0.0166 Epoch 5/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0177 - val_loss: 0.0171 Epoch 6/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0167 - val_loss: 0.0149 Epoch 7/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0159 - val_loss: 0.0146 Epoch 8/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0153 - val_loss: 0.0144 Epoch 9/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0146 - val_loss: 0.0134 Epoch 10/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0141 - val_loss: 0.0130 Epoch 11/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0136 - val_loss: 0.0127 Epoch 12/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0132 - val_loss: 0.0117 Epoch 13/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0128 - val_loss: 0.0113 Epoch 14/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0123 - val_loss: 0.0109 Epoch 15/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0118 - val_loss: 0.0095 Epoch 16/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0113 - val_loss: 0.0101 Epoch 17/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0109 - val_loss: 0.0080 Epoch 18/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0104 - val_loss: 0.0078 Epoch 19/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0100 - val_loss: 0.0079 Epoch 20/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0097 - val_loss: 0.0077 Epoch 21/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0093 - val_loss: 0.0067 Epoch 22/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0091 - val_loss: 0.0069 Epoch 23/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0089 - val_loss: 0.0070 Epoch 24/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0087 - val_loss: 0.0073 Epoch 25/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0085 - val_loss: 0.0065 Epoch 26/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0083 - val_loss: 0.0066 Epoch 27/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0083 - val_loss: 0.0070 Epoch 28/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0081 - val_loss: 0.0072 Epoch 29/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0080 - val_loss: 0.0072 Epoch 30/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0079 - val_loss: 0.0065 Epoch 31/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0078 - val_loss: 0.0065 Epoch 32/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0077 - val_loss: 0.0074 Epoch 33/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0077 - val_loss: 0.0058 Epoch 34/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0075 - val_loss: 0.0067 Epoch 35/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0074 - val_loss: 0.0068 Epoch 36/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0073 - val_loss: 0.0067 Epoch 37/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0073 - val_loss: 0.0059 Epoch 38/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0072 - val_loss: 0.0068 Epoch 39/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0072 - val_loss: 0.0065 Epoch 40/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0071 - val_loss: 0.0067 Epoch 41/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0070 - val_loss: 0.0058 Epoch 42/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0070 - val_loss: 0.0062 Epoch 43/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0069 - val_loss: 0.0060 Epoch 44/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0069 - val_loss: 0.0061 Epoch 45/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0069 - val_loss: 0.0068 Epoch 46/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0068 - val_loss: 0.0065 Epoch 47/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0067 - val_loss: 0.0061 Epoch 48/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0067 - val_loss: 0.0059 Epoch 49/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0066 - val_loss: 0.0060 Epoch 50/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0066 - val_loss: 0.0060 Epoch 51/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0066 - val_loss: 0.0060 Epoch 52/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0065 - val_loss: 0.0061 Epoch 53/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0065 - val_loss: 0.0061 Epoch 54/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0065 - val_loss: 0.0058 Epoch 55/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0064 - val_loss: 0.0056 Epoch 56/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0064 - val_loss: 0.0059 Epoch 57/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0064 - val_loss: 0.0061 Epoch 58/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0064 - val_loss: 0.0051 Epoch 59/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0064 - val_loss: 0.0058 Epoch 60/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0063 - val_loss: 0.0056 Epoch 61/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0063 - val_loss: 0.0057 Epoch 62/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0062 - val_loss: 0.0056 Epoch 63/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0062 - val_loss: 0.0053 Epoch 64/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0062 - val_loss: 0.0051 Epoch 65/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0062 - val_loss: 0.0056 Epoch 66/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0061 - val_loss: 0.0055 Epoch 67/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0061 - val_loss: 0.0055 Epoch 68/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0060 - val_loss: 0.0058 Epoch 69/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0060 - val_loss: 0.0058 Epoch 70/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0060 - val_loss: 0.0054 Epoch 71/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0060 - val_loss: 0.0049 Epoch 72/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0059 - val_loss: 0.0054 Epoch 73/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0059 - val_loss: 0.0052 Epoch 74/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0059 - val_loss: 0.0058 Epoch 75/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0059 - val_loss: 0.0056 Epoch 76/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0058 - val_loss: 0.0051 Epoch 77/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0058 - val_loss: 0.0054 Epoch 78/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0058 - val_loss: 0.0056 Epoch 79/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0058 - val_loss: 0.0051 Epoch 80/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0057 - val_loss: 0.0051 Epoch 81/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0057 - val_loss: 0.0055 Epoch 82/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0056 - val_loss: 0.0053 Epoch 83/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0057 - val_loss: 0.0055 Epoch 84/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0056 - val_loss: 0.0047 Epoch 85/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0056 - val_loss: 0.0049 Epoch 86/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0056 - val_loss: 0.0051 Epoch 87/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0055 - val_loss: 0.0051 Epoch 88/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0055 - val_loss: 0.0047 Epoch 89/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0054 - val_loss: 0.0054 Epoch 90/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0054 - val_loss: 0.0052 Epoch 91/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0054 - val_loss: 0.0053 Epoch 92/100 20/20 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0050 Epoch 93/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0053 - val_loss: 0.0050 Epoch 94/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0053 - val_loss: 0.0048 Epoch 95/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0052 - val_loss: 0.0047 Epoch 96/100 20/20 [==============================] - 0s 13ms/step - loss: 0.0052 - val_loss: 0.0049 Epoch 97/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0051 - val_loss: 0.0048 Epoch 98/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0052 - val_loss: 0.0046 Epoch 99/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0051 - val_loss: 0.0043 Epoch 100/100 20/20 [==============================] - 0s 12ms/step - loss: 0.0050 - val_loss: 0.0046 INFO:tensorflow:Assets written to: best_web_model/assets Validation mse: 0.004580171778798103 idx = np . random . choice ( 878 ) + 1 print ( \"Doe_ID\" , idx ) reg . compare ( idx ) Doe_ID 389 % matplotlib inline plt . rcParams [ 'figure.dpi' ] = 150 reg . interactive () interactive(children=(Dropdown(description='Material_ID', options=(1, 2, 3, 4, 5, 6), value=1), FloatSlider(va\u2026","title":"Web cut"},{"location":"Cut_x0/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); x0 cut prediction # import numpy as np import pandas as pd import matplotlib.pyplot as plt plt . rcParams [ 'figure.dpi' ] = 150 Loading the data # doe = pd . read_csv ( '../data/doe.csv' ) doe .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id Material_ID Blechdicke Niederhalterkraft Ziehspalt Stempel_ID Einlegeposition Ziehtiefe Breite UG OG E Material_Name Rp0 Rp0.2 Rp100 Rp25 Rp50 0 1 3 1.01 410 2.4 3 -5 30 70.2 1.71 2.00 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 1 2 5 1.48 400 2.4 3 -4 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 2 3 1 1.00 210 2.4 3 -1 70 70.2 1.71 2.00 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 3 4 2 1.19 390 2.4 3 5 30 70.2 1.71 2.00 200.03813 DC01_1.20mm 160.714935 174.535075 564.455438 424.931018 490.442223 4 5 1 1.00 360 1.6 2 3 30 71.8 1.14 1.33 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 995 996 3 1.01 90 1.6 2 0 70 71.8 1.14 1.33 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 996 997 4 0.99 480 2.4 3 5 30 70.2 1.71 2.00 71.94184 AA5083_1.00mm 248.796491 251.315902 385.373316 371.774337 384.299873 997 998 5 1.48 490 2.4 3 3 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 998 999 5 1.48 120 2.4 3 -3 30 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 999 1000 5 1.48 150 2.4 3 -2 70 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 1000 rows \u00d7 18 columns # Load the data using pandas data = pd . read_csv ( '../data/cut_x0_all.csv' ) data = data . head ( - 1000 ) # remove last experiment data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id y z x t deviation target_idx y_d z_d x_d t_d c color deviationc Ziehtiefe part tp 0 1 -94.864250 0.50 0.0 0.000000 0.543273 9 -94.855233 1.043198 0.0 0.000900 1.0 r 0.543273 30 lu 0.000000 1 1 -94.621034 0.50 0.0 0.001001 0.541849 19 -94.616495 1.041830 0.0 0.001900 1.0 r 0.541849 30 lu 0.004115 2 1 -94.377819 0.50 0.0 0.002002 0.540459 29 -94.377757 1.040459 0.0 0.002900 1.0 r 0.540459 30 lu 0.008230 3 1 -94.134603 0.50 0.0 0.003003 0.539104 39 -94.139019 1.039086 0.0 0.003900 1.0 r 0.539104 30 lu 0.012346 4 1 -93.891387 0.50 0.0 0.004004 0.537783 50 -93.876407 1.037574 0.0 0.005001 1.0 r 0.537783 30 lu 0.016461 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 879995 999 94.042937 0.75 0.0 0.995996 3.680685 9899 93.789254 4.421933 0.0 0.989999 1.0 cyan 3.680685 30 ru 4.983539 879996 999 94.286153 0.75 0.0 0.996997 3.697178 9909 94.028139 4.438164 0.0 0.990999 1.0 cyan 3.697178 30 ru 4.987654 879997 999 94.529369 0.75 0.0 0.997998 3.713665 9919 94.267024 4.454387 0.0 0.991999 1.0 cyan 3.713665 30 ru 4.991769 879998 999 94.772584 0.75 0.0 0.998999 3.730136 9930 94.529798 4.472227 0.0 0.993099 1.0 cyan 3.730136 30 ru 4.995885 879999 999 95.015800 0.75 0.0 1.000000 3.746598 9940 94.768684 4.488440 0.0 0.994099 1.0 cyan 3.746598 30 ru 5.000000 880000 rows \u00d7 17 columns Creating the regressor # from cut_predictor import CutPredictor reg = CutPredictor () reg . load_data ( doe = doe , data = data , index = 'doe_id' , process_parameters = [ 'Blechdicke' , 'Niederhalterkraft' , 'Ziehspalt' , 'Einlegeposition' , 'Ziehtiefe' , 'Stempel_ID' , 'E' , 'Rp0' , 'Rp50' ], categorical = [ 'Ziehspalt' , 'Ziehtiefe' , 'Stempel_ID' , ], position = 'tp' , output = [ 'deviationc' , 'y' , 'z' ] ) # Print a summary of the data reg . data_summary () Data summary ------------------------------------------------------------ Process parameters: - Blechdicke : numerical [ 0.99 ... 1.48 ] - Niederhalterkraft : numerical [ 10 ... 500 ] - Ziehspalt : categorical [1.6, 2.4] - Einlegeposition : numerical [ -5 ... 5 ] - Ziehtiefe : categorical [30, 50, 70] - Stempel_ID : categorical [2, 3] - E : numerical [ 71.68198000000001 ... 200.03813 ] - Rp0 : numerical [ 133.18263199999998 ... 296.5565 ] - Rp50 : numerical [ 383.0418340000001 ... 629.5304000000001 ] Position variable: - tp : numerical, [ 0.0 / 5.0 ] Output variable(s): - deviationc : numerical, [ -3.16506211149574 / 7.228601057768613 ] - y : numerical, [ -94.86425 / 95.0158 ] - z : numerical, [ 0.483866215444952 / 70.76854729051487 ] Inputs ------------------------------------------------------------ (880000, 14) Outputs ------------------------------------------------------------ (880000, 3) Training methods # Autotuning # best_config = reg . autotune ( save_path = 'best_x0_model' , trials = 100 , max_epochs = 20 , layers = [ 4 , 6 ], neurons = [ 64 , 256 , 64 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-5 , 1e-3 ] ) reg . training_summary () Alternative: define a custom network and do the optimization yourself # One can also run the autotuning for a limited number of epochs and then fine-tune the best configuration by training it longer. config = { 'batch_size' : 4096 , 'max_epochs' : 50 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.0 , 'learning_rate' : 0.001 } # or best_config from autotune if you already did it once reg . custom_model ( save_path = 'best_x0_model' , config = config , verbose = True ) reg . training_summary () Metal device set to: Apple M1 Pro systemMemory: 16.00 GB maxCacheSize: 5.33 GB Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 128) 1920 dense_1 (Dense) (None, 128) 16512 dense_2 (Dense) (None, 128) 16512 dense_3 (Dense) (None, 128) 16512 dense_4 (Dense) (None, 128) 16512 dense_5 (Dense) (None, 3) 387 ================================================================= Total params: 68,355 Trainable params: 68,355 Non-trainable params: 0 _________________________________________________________________ Epoch 1/50 194/194 [==============================] - 2s 10ms/step - loss: 0.0112 - val_loss: 6.9350e-04 Epoch 2/50 194/194 [==============================] - 2s 9ms/step - loss: 4.4537e-04 - val_loss: 3.9001e-04 Epoch 3/50 194/194 [==============================] - 2s 10ms/step - loss: 3.0705e-04 - val_loss: 2.9443e-04 Epoch 4/50 194/194 [==============================] - 2s 10ms/step - loss: 2.4155e-04 - val_loss: 2.4613e-04 Epoch 5/50 194/194 [==============================] - 2s 10ms/step - loss: 2.1204e-04 - val_loss: 2.5926e-04 Epoch 6/50 194/194 [==============================] - 2s 10ms/step - loss: 1.9883e-04 - val_loss: 2.3161e-04 Epoch 7/50 194/194 [==============================] - 2s 10ms/step - loss: 1.6883e-04 - val_loss: 1.7670e-04 Epoch 8/50 194/194 [==============================] - 2s 9ms/step - loss: 1.5758e-04 - val_loss: 1.7490e-04 Epoch 9/50 194/194 [==============================] - 2s 10ms/step - loss: 1.5062e-04 - val_loss: 1.8632e-04 Epoch 10/50 194/194 [==============================] - 2s 10ms/step - loss: 1.3692e-04 - val_loss: 1.4604e-04 Epoch 11/50 194/194 [==============================] - 2s 10ms/step - loss: 1.2795e-04 - val_loss: 1.6347e-04 Epoch 12/50 194/194 [==============================] - 2s 10ms/step - loss: 1.1989e-04 - val_loss: 1.6298e-04 Epoch 13/50 194/194 [==============================] - 2s 10ms/step - loss: 1.1915e-04 - val_loss: 2.2435e-04 Epoch 14/50 194/194 [==============================] - 2s 10ms/step - loss: 1.0475e-04 - val_loss: 1.4843e-04 Epoch 15/50 194/194 [==============================] - 2s 10ms/step - loss: 1.0382e-04 - val_loss: 1.0533e-04 Epoch 16/50 194/194 [==============================] - 2s 10ms/step - loss: 9.8808e-05 - val_loss: 1.3266e-04 Epoch 17/50 194/194 [==============================] - 2s 10ms/step - loss: 9.5478e-05 - val_loss: 1.1126e-04 Epoch 18/50 194/194 [==============================] - 2s 11ms/step - loss: 9.5013e-05 - val_loss: 1.2046e-04 Epoch 19/50 194/194 [==============================] - 2s 10ms/step - loss: 8.4487e-05 - val_loss: 1.0094e-04 Epoch 20/50 194/194 [==============================] - 2s 10ms/step - loss: 8.3769e-05 - val_loss: 1.0605e-04 Epoch 21/50 194/194 [==============================] - 2s 10ms/step - loss: 8.4088e-05 - val_loss: 1.2499e-04 Epoch 22/50 194/194 [==============================] - 2s 10ms/step - loss: 7.5956e-05 - val_loss: 1.2025e-04 Epoch 23/50 194/194 [==============================] - 2s 10ms/step - loss: 7.6716e-05 - val_loss: 9.0917e-05 Epoch 24/50 194/194 [==============================] - 2s 10ms/step - loss: 7.6298e-05 - val_loss: 1.4897e-04 Epoch 25/50 194/194 [==============================] - 2s 10ms/step - loss: 7.1159e-05 - val_loss: 8.2532e-05 Epoch 26/50 194/194 [==============================] - 2s 9ms/step - loss: 7.1947e-05 - val_loss: 8.3932e-05 Epoch 27/50 194/194 [==============================] - 2s 10ms/step - loss: 6.5481e-05 - val_loss: 1.7676e-04 Epoch 28/50 194/194 [==============================] - 2s 10ms/step - loss: 6.7114e-05 - val_loss: 8.0728e-05 Epoch 29/50 194/194 [==============================] - 2s 10ms/step - loss: 6.3867e-05 - val_loss: 8.2021e-05 Epoch 30/50 194/194 [==============================] - 2s 9ms/step - loss: 6.6426e-05 - val_loss: 1.0495e-04 Epoch 31/50 194/194 [==============================] - 2s 10ms/step - loss: 5.9351e-05 - val_loss: 1.0094e-04 Epoch 32/50 194/194 [==============================] - 2s 10ms/step - loss: 6.3574e-05 - val_loss: 9.5542e-05 Epoch 33/50 194/194 [==============================] - 2s 10ms/step - loss: 6.1393e-05 - val_loss: 1.4148e-04 Epoch 34/50 194/194 [==============================] - 2s 9ms/step - loss: 5.7842e-05 - val_loss: 9.7597e-05 Epoch 35/50 194/194 [==============================] - 2s 10ms/step - loss: 5.5261e-05 - val_loss: 9.9637e-05 Epoch 36/50 194/194 [==============================] - 2s 10ms/step - loss: 5.5943e-05 - val_loss: 1.0083e-04 Epoch 37/50 194/194 [==============================] - 2s 10ms/step - loss: 5.5862e-05 - val_loss: 7.7576e-05 Epoch 38/50 194/194 [==============================] - 2s 10ms/step - loss: 5.9933e-05 - val_loss: 7.6082e-05 Epoch 39/50 194/194 [==============================] - 2s 9ms/step - loss: 5.0190e-05 - val_loss: 6.5452e-05 Epoch 40/50 194/194 [==============================] - 2s 9ms/step - loss: 5.1849e-05 - val_loss: 1.1664e-04 Epoch 41/50 194/194 [==============================] - 2s 10ms/step - loss: 5.1149e-05 - val_loss: 9.6775e-05 Epoch 42/50 194/194 [==============================] - 2s 9ms/step - loss: 5.1867e-05 - val_loss: 1.1928e-04 Epoch 43/50 194/194 [==============================] - 2s 10ms/step - loss: 4.7170e-05 - val_loss: 6.5815e-05 Epoch 44/50 194/194 [==============================] - 2s 10ms/step - loss: 4.7070e-05 - val_loss: 8.1336e-05 Epoch 45/50 194/194 [==============================] - 2s 10ms/step - loss: 4.6246e-05 - val_loss: 7.0416e-05 Epoch 46/50 194/194 [==============================] - 2s 10ms/step - loss: 4.4535e-05 - val_loss: 1.0595e-04 Epoch 47/50 194/194 [==============================] - 2s 10ms/step - loss: 4.4189e-05 - val_loss: 1.0225e-04 Epoch 48/50 194/194 [==============================] - 2s 10ms/step - loss: 4.4210e-05 - val_loss: 6.5181e-05 Epoch 49/50 194/194 [==============================] - 2s 10ms/step - loss: 4.4608e-05 - val_loss: 6.0032e-05 Epoch 50/50 194/194 [==============================] - 2s 9ms/step - loss: 4.1002e-05 - val_loss: 6.8833e-05 INFO:tensorflow:Assets written to: best_x0_model/assets Validation mse: 6.883306195959449e-05 Other alternative: the model has already been trained # We just need to reload it to make predictions. reg . load ( load_path = 'best_x0_model' ) Visualization # Prediction for single process parameter values # x , y = reg . predict ({ 'Blechdicke' : 1.01 , 'Niederhalterkraft' : 410.0 , 'Ziehspalt' : 2.4 , 'Einlegeposition' : - 5 , 'Ziehtiefe' : 30 , 'Stempel_ID' : 3 , 'E' : 191.37245 , 'Rp0' : 138.22696 , 'Rp50' : 449.528189 , }, nb_points = 1000 ) plt . figure () plt . plot ( x , y [:, 0 ]) plt . xlabel ( 'tp' ) plt . ylabel ( 'deviationc' ) plt . figure () plt . plot ( y [:, 1 ], y [:, 2 ]) plt . xlabel ( 'y' ) plt . ylabel ( 'z' ) Text(0, 0.5, 'z') Comparison with the ground truth on the training set # Randomly choose an id between 1 and 1000 and compare the prediction to the ground truth. If the experiment does not exist in the data, an error will be thrown. idx = np . random . choice ( 878 ) + 1 print ( \"Doe_ID\" , idx ) reg . compare ( idx ) Doe_ID 349 % matplotlib inline plt . rcParams [ 'figure.dpi' ] = 150 reg . interactive () interactive(children=(FloatSlider(value=1.1883700000000001, description='Blechdicke', max=1.48, min=0.99, step\u2026","title":"Cut x0"},{"location":"Cut_x0/#x0-cut-prediction","text":"import numpy as np import pandas as pd import matplotlib.pyplot as plt plt . rcParams [ 'figure.dpi' ] = 150","title":"x0 cut prediction"},{"location":"Cut_x0/#loading-the-data","text":"doe = pd . read_csv ( '../data/doe.csv' ) doe .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id Material_ID Blechdicke Niederhalterkraft Ziehspalt Stempel_ID Einlegeposition Ziehtiefe Breite UG OG E Material_Name Rp0 Rp0.2 Rp100 Rp25 Rp50 0 1 3 1.01 410 2.4 3 -5 30 70.2 1.71 2.00 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 1 2 5 1.48 400 2.4 3 -4 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 2 3 1 1.00 210 2.4 3 -1 70 70.2 1.71 2.00 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 3 4 2 1.19 390 2.4 3 5 30 70.2 1.71 2.00 200.03813 DC01_1.20mm 160.714935 174.535075 564.455438 424.931018 490.442223 4 5 1 1.00 360 1.6 2 3 30 71.8 1.14 1.33 164.62254 DC01_1.00mm 133.182632 147.101263 517.275855 385.773439 447.384736 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 995 996 3 1.01 90 1.6 2 0 70 71.8 1.14 1.33 191.37245 DC04_1.00mm 138.226960 147.601859 534.002871 377.443009 449.528189 996 997 4 0.99 480 2.4 3 5 30 70.2 1.71 2.00 71.94184 AA5083_1.00mm 248.796491 251.315902 385.373316 371.774337 384.299873 997 998 5 1.48 490 2.4 3 3 50 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 998 999 5 1.48 120 2.4 3 -3 30 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 999 1000 5 1.48 150 2.4 3 -2 70 70.2 1.71 2.00 71.68198 AA5083_1.50mm 235.160326 237.951493 384.123147 369.820589 383.041834 1000 rows \u00d7 18 columns # Load the data using pandas data = pd . read_csv ( '../data/cut_x0_all.csv' ) data = data . head ( - 1000 ) # remove last experiment data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } doe_id y z x t deviation target_idx y_d z_d x_d t_d c color deviationc Ziehtiefe part tp 0 1 -94.864250 0.50 0.0 0.000000 0.543273 9 -94.855233 1.043198 0.0 0.000900 1.0 r 0.543273 30 lu 0.000000 1 1 -94.621034 0.50 0.0 0.001001 0.541849 19 -94.616495 1.041830 0.0 0.001900 1.0 r 0.541849 30 lu 0.004115 2 1 -94.377819 0.50 0.0 0.002002 0.540459 29 -94.377757 1.040459 0.0 0.002900 1.0 r 0.540459 30 lu 0.008230 3 1 -94.134603 0.50 0.0 0.003003 0.539104 39 -94.139019 1.039086 0.0 0.003900 1.0 r 0.539104 30 lu 0.012346 4 1 -93.891387 0.50 0.0 0.004004 0.537783 50 -93.876407 1.037574 0.0 0.005001 1.0 r 0.537783 30 lu 0.016461 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 879995 999 94.042937 0.75 0.0 0.995996 3.680685 9899 93.789254 4.421933 0.0 0.989999 1.0 cyan 3.680685 30 ru 4.983539 879996 999 94.286153 0.75 0.0 0.996997 3.697178 9909 94.028139 4.438164 0.0 0.990999 1.0 cyan 3.697178 30 ru 4.987654 879997 999 94.529369 0.75 0.0 0.997998 3.713665 9919 94.267024 4.454387 0.0 0.991999 1.0 cyan 3.713665 30 ru 4.991769 879998 999 94.772584 0.75 0.0 0.998999 3.730136 9930 94.529798 4.472227 0.0 0.993099 1.0 cyan 3.730136 30 ru 4.995885 879999 999 95.015800 0.75 0.0 1.000000 3.746598 9940 94.768684 4.488440 0.0 0.994099 1.0 cyan 3.746598 30 ru 5.000000 880000 rows \u00d7 17 columns","title":"Loading the data"},{"location":"Cut_x0/#creating-the-regressor","text":"from cut_predictor import CutPredictor reg = CutPredictor () reg . load_data ( doe = doe , data = data , index = 'doe_id' , process_parameters = [ 'Blechdicke' , 'Niederhalterkraft' , 'Ziehspalt' , 'Einlegeposition' , 'Ziehtiefe' , 'Stempel_ID' , 'E' , 'Rp0' , 'Rp50' ], categorical = [ 'Ziehspalt' , 'Ziehtiefe' , 'Stempel_ID' , ], position = 'tp' , output = [ 'deviationc' , 'y' , 'z' ] ) # Print a summary of the data reg . data_summary () Data summary ------------------------------------------------------------ Process parameters: - Blechdicke : numerical [ 0.99 ... 1.48 ] - Niederhalterkraft : numerical [ 10 ... 500 ] - Ziehspalt : categorical [1.6, 2.4] - Einlegeposition : numerical [ -5 ... 5 ] - Ziehtiefe : categorical [30, 50, 70] - Stempel_ID : categorical [2, 3] - E : numerical [ 71.68198000000001 ... 200.03813 ] - Rp0 : numerical [ 133.18263199999998 ... 296.5565 ] - Rp50 : numerical [ 383.0418340000001 ... 629.5304000000001 ] Position variable: - tp : numerical, [ 0.0 / 5.0 ] Output variable(s): - deviationc : numerical, [ -3.16506211149574 / 7.228601057768613 ] - y : numerical, [ -94.86425 / 95.0158 ] - z : numerical, [ 0.483866215444952 / 70.76854729051487 ] Inputs ------------------------------------------------------------ (880000, 14) Outputs ------------------------------------------------------------ (880000, 3)","title":"Creating the regressor"},{"location":"Cut_x0/#training-methods","text":"","title":"Training methods"},{"location":"Cut_x0/#autotuning","text":"best_config = reg . autotune ( save_path = 'best_x0_model' , trials = 100 , max_epochs = 20 , layers = [ 4 , 6 ], neurons = [ 64 , 256 , 64 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-5 , 1e-3 ] ) reg . training_summary ()","title":"Autotuning"},{"location":"Cut_x0/#alternative-define-a-custom-network-and-do-the-optimization-yourself","text":"One can also run the autotuning for a limited number of epochs and then fine-tune the best configuration by training it longer. config = { 'batch_size' : 4096 , 'max_epochs' : 50 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.0 , 'learning_rate' : 0.001 } # or best_config from autotune if you already did it once reg . custom_model ( save_path = 'best_x0_model' , config = config , verbose = True ) reg . training_summary () Metal device set to: Apple M1 Pro systemMemory: 16.00 GB maxCacheSize: 5.33 GB Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 128) 1920 dense_1 (Dense) (None, 128) 16512 dense_2 (Dense) (None, 128) 16512 dense_3 (Dense) (None, 128) 16512 dense_4 (Dense) (None, 128) 16512 dense_5 (Dense) (None, 3) 387 ================================================================= Total params: 68,355 Trainable params: 68,355 Non-trainable params: 0 _________________________________________________________________ Epoch 1/50 194/194 [==============================] - 2s 10ms/step - loss: 0.0112 - val_loss: 6.9350e-04 Epoch 2/50 194/194 [==============================] - 2s 9ms/step - loss: 4.4537e-04 - val_loss: 3.9001e-04 Epoch 3/50 194/194 [==============================] - 2s 10ms/step - loss: 3.0705e-04 - val_loss: 2.9443e-04 Epoch 4/50 194/194 [==============================] - 2s 10ms/step - loss: 2.4155e-04 - val_loss: 2.4613e-04 Epoch 5/50 194/194 [==============================] - 2s 10ms/step - loss: 2.1204e-04 - val_loss: 2.5926e-04 Epoch 6/50 194/194 [==============================] - 2s 10ms/step - loss: 1.9883e-04 - val_loss: 2.3161e-04 Epoch 7/50 194/194 [==============================] - 2s 10ms/step - loss: 1.6883e-04 - val_loss: 1.7670e-04 Epoch 8/50 194/194 [==============================] - 2s 9ms/step - loss: 1.5758e-04 - val_loss: 1.7490e-04 Epoch 9/50 194/194 [==============================] - 2s 10ms/step - loss: 1.5062e-04 - val_loss: 1.8632e-04 Epoch 10/50 194/194 [==============================] - 2s 10ms/step - loss: 1.3692e-04 - val_loss: 1.4604e-04 Epoch 11/50 194/194 [==============================] - 2s 10ms/step - loss: 1.2795e-04 - val_loss: 1.6347e-04 Epoch 12/50 194/194 [==============================] - 2s 10ms/step - loss: 1.1989e-04 - val_loss: 1.6298e-04 Epoch 13/50 194/194 [==============================] - 2s 10ms/step - loss: 1.1915e-04 - val_loss: 2.2435e-04 Epoch 14/50 194/194 [==============================] - 2s 10ms/step - loss: 1.0475e-04 - val_loss: 1.4843e-04 Epoch 15/50 194/194 [==============================] - 2s 10ms/step - loss: 1.0382e-04 - val_loss: 1.0533e-04 Epoch 16/50 194/194 [==============================] - 2s 10ms/step - loss: 9.8808e-05 - val_loss: 1.3266e-04 Epoch 17/50 194/194 [==============================] - 2s 10ms/step - loss: 9.5478e-05 - val_loss: 1.1126e-04 Epoch 18/50 194/194 [==============================] - 2s 11ms/step - loss: 9.5013e-05 - val_loss: 1.2046e-04 Epoch 19/50 194/194 [==============================] - 2s 10ms/step - loss: 8.4487e-05 - val_loss: 1.0094e-04 Epoch 20/50 194/194 [==============================] - 2s 10ms/step - loss: 8.3769e-05 - val_loss: 1.0605e-04 Epoch 21/50 194/194 [==============================] - 2s 10ms/step - loss: 8.4088e-05 - val_loss: 1.2499e-04 Epoch 22/50 194/194 [==============================] - 2s 10ms/step - loss: 7.5956e-05 - val_loss: 1.2025e-04 Epoch 23/50 194/194 [==============================] - 2s 10ms/step - loss: 7.6716e-05 - val_loss: 9.0917e-05 Epoch 24/50 194/194 [==============================] - 2s 10ms/step - loss: 7.6298e-05 - val_loss: 1.4897e-04 Epoch 25/50 194/194 [==============================] - 2s 10ms/step - loss: 7.1159e-05 - val_loss: 8.2532e-05 Epoch 26/50 194/194 [==============================] - 2s 9ms/step - loss: 7.1947e-05 - val_loss: 8.3932e-05 Epoch 27/50 194/194 [==============================] - 2s 10ms/step - loss: 6.5481e-05 - val_loss: 1.7676e-04 Epoch 28/50 194/194 [==============================] - 2s 10ms/step - loss: 6.7114e-05 - val_loss: 8.0728e-05 Epoch 29/50 194/194 [==============================] - 2s 10ms/step - loss: 6.3867e-05 - val_loss: 8.2021e-05 Epoch 30/50 194/194 [==============================] - 2s 9ms/step - loss: 6.6426e-05 - val_loss: 1.0495e-04 Epoch 31/50 194/194 [==============================] - 2s 10ms/step - loss: 5.9351e-05 - val_loss: 1.0094e-04 Epoch 32/50 194/194 [==============================] - 2s 10ms/step - loss: 6.3574e-05 - val_loss: 9.5542e-05 Epoch 33/50 194/194 [==============================] - 2s 10ms/step - loss: 6.1393e-05 - val_loss: 1.4148e-04 Epoch 34/50 194/194 [==============================] - 2s 9ms/step - loss: 5.7842e-05 - val_loss: 9.7597e-05 Epoch 35/50 194/194 [==============================] - 2s 10ms/step - loss: 5.5261e-05 - val_loss: 9.9637e-05 Epoch 36/50 194/194 [==============================] - 2s 10ms/step - loss: 5.5943e-05 - val_loss: 1.0083e-04 Epoch 37/50 194/194 [==============================] - 2s 10ms/step - loss: 5.5862e-05 - val_loss: 7.7576e-05 Epoch 38/50 194/194 [==============================] - 2s 10ms/step - loss: 5.9933e-05 - val_loss: 7.6082e-05 Epoch 39/50 194/194 [==============================] - 2s 9ms/step - loss: 5.0190e-05 - val_loss: 6.5452e-05 Epoch 40/50 194/194 [==============================] - 2s 9ms/step - loss: 5.1849e-05 - val_loss: 1.1664e-04 Epoch 41/50 194/194 [==============================] - 2s 10ms/step - loss: 5.1149e-05 - val_loss: 9.6775e-05 Epoch 42/50 194/194 [==============================] - 2s 9ms/step - loss: 5.1867e-05 - val_loss: 1.1928e-04 Epoch 43/50 194/194 [==============================] - 2s 10ms/step - loss: 4.7170e-05 - val_loss: 6.5815e-05 Epoch 44/50 194/194 [==============================] - 2s 10ms/step - loss: 4.7070e-05 - val_loss: 8.1336e-05 Epoch 45/50 194/194 [==============================] - 2s 10ms/step - loss: 4.6246e-05 - val_loss: 7.0416e-05 Epoch 46/50 194/194 [==============================] - 2s 10ms/step - loss: 4.4535e-05 - val_loss: 1.0595e-04 Epoch 47/50 194/194 [==============================] - 2s 10ms/step - loss: 4.4189e-05 - val_loss: 1.0225e-04 Epoch 48/50 194/194 [==============================] - 2s 10ms/step - loss: 4.4210e-05 - val_loss: 6.5181e-05 Epoch 49/50 194/194 [==============================] - 2s 10ms/step - loss: 4.4608e-05 - val_loss: 6.0032e-05 Epoch 50/50 194/194 [==============================] - 2s 9ms/step - loss: 4.1002e-05 - val_loss: 6.8833e-05 INFO:tensorflow:Assets written to: best_x0_model/assets Validation mse: 6.883306195959449e-05","title":"Alternative: define a custom network and do the optimization yourself"},{"location":"Cut_x0/#other-alternative-the-model-has-already-been-trained","text":"We just need to reload it to make predictions. reg . load ( load_path = 'best_x0_model' )","title":"Other alternative: the model has already been trained"},{"location":"Cut_x0/#visualization","text":"","title":"Visualization"},{"location":"Cut_x0/#prediction-for-single-process-parameter-values","text":"x , y = reg . predict ({ 'Blechdicke' : 1.01 , 'Niederhalterkraft' : 410.0 , 'Ziehspalt' : 2.4 , 'Einlegeposition' : - 5 , 'Ziehtiefe' : 30 , 'Stempel_ID' : 3 , 'E' : 191.37245 , 'Rp0' : 138.22696 , 'Rp50' : 449.528189 , }, nb_points = 1000 ) plt . figure () plt . plot ( x , y [:, 0 ]) plt . xlabel ( 'tp' ) plt . ylabel ( 'deviationc' ) plt . figure () plt . plot ( y [:, 1 ], y [:, 2 ]) plt . xlabel ( 'y' ) plt . ylabel ( 'z' ) Text(0, 0.5, 'z')","title":"Prediction for single process parameter values"},{"location":"Cut_x0/#comparison-with-the-ground-truth-on-the-training-set","text":"Randomly choose an id between 1 and 1000 and compare the prediction to the ground truth. If the experiment does not exist in the data, an error will be thrown. idx = np . random . choice ( 878 ) + 1 print ( \"Doe_ID\" , idx ) reg . compare ( idx ) Doe_ID 349 % matplotlib inline plt . rcParams [ 'figure.dpi' ] = 150 reg . interactive () interactive(children=(FloatSlider(value=1.1883700000000001, description='Blechdicke', max=1.48, min=0.99, step\u2026","title":"Comparison with the ground truth on the training set"},{"location":"datapreparation/","text":"Data preparation # The data should be passed to the CutPredictor as pandas data frames. Each experiment (or simulation) with a unique ID 'doe_id' consists of: a set of \\(d\\) process parameters defining the experiment (definition of the material, force applied, etc). a 1D position regularly sampled along some axis ( \\(m\\) points). a resulting output variable (such as a deviation) for each position. The goal is for example to learn to predict the deviation for any position, given a set of (new) process parameters. Taking the example of the x0 cut in the Cut_x0.ipynb notebook, the process parameters may be: Blechdicke Niederhalterkraft Ziehspalt Einlegeposition Ziehtiefe the position is tp and the value to predict is deviationc . Two data frames should be prepared: A doe data frame for the \\(d\\) process parameters, including a unique ID per experiment. A data data frame for the 1D values, with a unique experiment ID for each sample. The doe data frame should look like: doe_id Blechdicke Kraft Ziehspalt Position Ziehtiefe 1 1.01 410.0 2.4 -5 30 2 1.48 400.0 2.4 -4 30 3 1.00 210.0 2.4 -1 30 ... ... ... ... ... ... The data data frame should look like: doe_id tp deviationc 1 0.00 1.43 1 0.01 1.45 1 0.02 1.47 ... ... ... Additional columns are not a problem. The name of the column for the experiment ID (here doe_id ) should be the same in both data frames. Once the data is prepared in that format and saved to disk (csv, hdf5...), the data frames can be loaded: doe = pd . read_csv ( 'doe.csv' ) data = pd . read_csv ( 'data.csv' )","title":"Data preparation"},{"location":"datapreparation/#data-preparation","text":"The data should be passed to the CutPredictor as pandas data frames. Each experiment (or simulation) with a unique ID 'doe_id' consists of: a set of \\(d\\) process parameters defining the experiment (definition of the material, force applied, etc). a 1D position regularly sampled along some axis ( \\(m\\) points). a resulting output variable (such as a deviation) for each position. The goal is for example to learn to predict the deviation for any position, given a set of (new) process parameters. Taking the example of the x0 cut in the Cut_x0.ipynb notebook, the process parameters may be: Blechdicke Niederhalterkraft Ziehspalt Einlegeposition Ziehtiefe the position is tp and the value to predict is deviationc . Two data frames should be prepared: A doe data frame for the \\(d\\) process parameters, including a unique ID per experiment. A data data frame for the 1D values, with a unique experiment ID for each sample. The doe data frame should look like: doe_id Blechdicke Kraft Ziehspalt Position Ziehtiefe 1 1.01 410.0 2.4 -5 30 2 1.48 400.0 2.4 -4 30 3 1.00 210.0 2.4 -1 30 ... ... ... ... ... ... The data data frame should look like: doe_id tp deviationc 1 0.00 1.43 1 0.01 1.45 1 0.02 1.47 ... ... ... Additional columns are not a problem. The name of the column for the experiment ID (here doe_id ) should be the same in both data frames. Once the data is prepared in that format and saved to disk (csv, hdf5...), the data frames can be loaded: doe = pd . read_csv ( 'doe.csv' ) data = pd . read_csv ( 'data.csv' )","title":"Data preparation"},{"location":"usage/","text":"Usage # Loading the data # Once the data is loaded as pandas Dataframes, it can be loaded into the CutPredictor object: reg = CutPredictor () reg . load_data ( doe = doe , data = data , process_parameters = [ 'Blechdicke' , 'Niederhalterkraft' , 'Ziehspalt' , 'Einlegeposition' , 'Ziehtiefe' ], categorical = [ 'Ziehspalt' , 'Einlegeposition' , 'Ziehtiefe' ], position = 'tp' , output = 'deviationc' , index = 'doe_id' , ) One has to specify among all the columns present in the frames which ones are process parameters, which one is the input position and which one(s) is the output. If several output variables should be predicted, a list of names should be given. The name of the column contining the experiment ID should be provided. Optionally, one can specify which process parameter is categorical (as opposed to numerical), i.e. takes discrete values. This only influences the training of the neural network, as categorical attributes are then one-hot encoded before being passed to the NN. This is however optional. When the data is loaded in the CutPredictor , it is normalized to allow for efficient training, so this can take a while depending on the size of your data. Training the network # CutPredictor uses a feedforward neural network to perform the regression. It is a simple multi-layer perceptron with several layers of neuron, using the mean square error as a loss function. The hyperparameters of the NN are: the number of layers and neurons. the learning rate. the dropout level for regularization. the batch size. the number of epochs It may be tricky to find the right hyperparameters for your data. Two methods are available: an automatic one using the optuna library and a manual one. Autotuning # The autotune method launches a Bayesian optimization procedure thanks to the optuna library to find the best set of hyperparameters for the data. In a nutshell, it will train trials=100 different networks with hyperparameters sampled ffrom the specified ranges: best_config = reg . autotune ( save_path = 'best_model' , trials = 100 , max_epochs = 50 , layers = [ 3 , 5 ], neurons = [ 64 , 256 , 64 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-5 , 1e-3 ] ) The networks have diffrent number of layers, neurons per layer, learning rates and so on. The network with the best validation accuracy is finally selected and saved in the best_model/ directory. It can be used to make predictions. The more trials you make, the more likely you will find a satisfying solution if the provided ranges are well chosen. But if the ranges are too wide, you will neeed many trials to find the optimal setup. The autotuning procedure can take a while depending on the number of trials, the size of the networks and the maximum number of epochs. If you have multiple GPUs on the system, try to limit tensorflow training to one of them (e.g. GPU of id 0). If your code is in a script, use: CUDA_VIDIBLE_DEVICES = 0 python Script . py In a notebook, run that cell first: import os os . environ [ \"CUDA_DEVICE_ORDER\" ] = \"PCI_BUS_ID\" os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"0\" Manual selection # If you prefer to do the optimization yourself, or fine-tune the architecture found by autotune by training it for more epochs, you can define the network using a dictionary: config = { 'batch_size' : 4096 , 'max_epochs' : 50 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.0 , 'learning_rate' : 0.005 } and train that model: reg . custom_model ( save_path = 'best_model' , config = config , verbose = True ) The model's weights are saved in best_model/ . Visualizing the results # To make a prediction for a (potentially new) set of process parameters, simply pass them to the predict() method as a dictionary: x , y = reg . predict ({ 'Blechdicke' : 1.01 , 'Niederhalterkraft' : 410.0 , 'Ziehspalt' : 2.4 , 'Einlegeposition' : - 5 , 'Ziehtiefe' : 30 }, nb_points = 1000 ) This will return 1000 values of the position x as well as the corresponding predicted output y . In a Jupyter notebook, you can use the interactive() method to get sliders for the process parameters and interactively visualize the predictions: % matplotlib inline # necessary plt . rcParams [ 'figure.dpi' ] = 150 # bigger figures reg . interactive () Loading a pretrained network # Once a suitable model has been trained on the data and saved in 'best_model', it can be used for inference. If you do not want the data to be loaded for the prediction, you should save the data-related parameters (min/max values of the attributes, etc) in a pickle file: reg . save_config ( \"network.pkl\" ) This way, you can recreate a CutPredictor object without loading the data in memory: reg = CutPredictor () reg . load_config ( \"network.pkl\" ) and finally load the weights of the trained model: reg . load ( load_path = 'best_model' )","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#loading-the-data","text":"Once the data is loaded as pandas Dataframes, it can be loaded into the CutPredictor object: reg = CutPredictor () reg . load_data ( doe = doe , data = data , process_parameters = [ 'Blechdicke' , 'Niederhalterkraft' , 'Ziehspalt' , 'Einlegeposition' , 'Ziehtiefe' ], categorical = [ 'Ziehspalt' , 'Einlegeposition' , 'Ziehtiefe' ], position = 'tp' , output = 'deviationc' , index = 'doe_id' , ) One has to specify among all the columns present in the frames which ones are process parameters, which one is the input position and which one(s) is the output. If several output variables should be predicted, a list of names should be given. The name of the column contining the experiment ID should be provided. Optionally, one can specify which process parameter is categorical (as opposed to numerical), i.e. takes discrete values. This only influences the training of the neural network, as categorical attributes are then one-hot encoded before being passed to the NN. This is however optional. When the data is loaded in the CutPredictor , it is normalized to allow for efficient training, so this can take a while depending on the size of your data.","title":"Loading the data"},{"location":"usage/#training-the-network","text":"CutPredictor uses a feedforward neural network to perform the regression. It is a simple multi-layer perceptron with several layers of neuron, using the mean square error as a loss function. The hyperparameters of the NN are: the number of layers and neurons. the learning rate. the dropout level for regularization. the batch size. the number of epochs It may be tricky to find the right hyperparameters for your data. Two methods are available: an automatic one using the optuna library and a manual one.","title":"Training the network"},{"location":"usage/#autotuning","text":"The autotune method launches a Bayesian optimization procedure thanks to the optuna library to find the best set of hyperparameters for the data. In a nutshell, it will train trials=100 different networks with hyperparameters sampled ffrom the specified ranges: best_config = reg . autotune ( save_path = 'best_model' , trials = 100 , max_epochs = 50 , layers = [ 3 , 5 ], neurons = [ 64 , 256 , 64 ], dropout = [ 0.0 , 0.5 , 0.1 ], learning_rate = [ 1e-5 , 1e-3 ] ) The networks have diffrent number of layers, neurons per layer, learning rates and so on. The network with the best validation accuracy is finally selected and saved in the best_model/ directory. It can be used to make predictions. The more trials you make, the more likely you will find a satisfying solution if the provided ranges are well chosen. But if the ranges are too wide, you will neeed many trials to find the optimal setup. The autotuning procedure can take a while depending on the number of trials, the size of the networks and the maximum number of epochs. If you have multiple GPUs on the system, try to limit tensorflow training to one of them (e.g. GPU of id 0). If your code is in a script, use: CUDA_VIDIBLE_DEVICES = 0 python Script . py In a notebook, run that cell first: import os os . environ [ \"CUDA_DEVICE_ORDER\" ] = \"PCI_BUS_ID\" os . environ [ \"CUDA_VISIBLE_DEVICES\" ] = \"0\"","title":"Autotuning"},{"location":"usage/#manual-selection","text":"If you prefer to do the optimization yourself, or fine-tune the architecture found by autotune by training it for more epochs, you can define the network using a dictionary: config = { 'batch_size' : 4096 , 'max_epochs' : 50 , 'layers' : [ 128 , 128 , 128 , 128 , 128 ], 'dropout' : 0.0 , 'learning_rate' : 0.005 } and train that model: reg . custom_model ( save_path = 'best_model' , config = config , verbose = True ) The model's weights are saved in best_model/ .","title":"Manual selection"},{"location":"usage/#visualizing-the-results","text":"To make a prediction for a (potentially new) set of process parameters, simply pass them to the predict() method as a dictionary: x , y = reg . predict ({ 'Blechdicke' : 1.01 , 'Niederhalterkraft' : 410.0 , 'Ziehspalt' : 2.4 , 'Einlegeposition' : - 5 , 'Ziehtiefe' : 30 }, nb_points = 1000 ) This will return 1000 values of the position x as well as the corresponding predicted output y . In a Jupyter notebook, you can use the interactive() method to get sliders for the process parameters and interactively visualize the predictions: % matplotlib inline # necessary plt . rcParams [ 'figure.dpi' ] = 150 # bigger figures reg . interactive ()","title":"Visualizing the results"},{"location":"usage/#loading-a-pretrained-network","text":"Once a suitable model has been trained on the data and saved in 'best_model', it can be used for inference. If you do not want the data to be loaded for the prediction, you should save the data-related parameters (min/max values of the attributes, etc) in a pickle file: reg . save_config ( \"network.pkl\" ) This way, you can recreate a CutPredictor object without loading the data in memory: reg = CutPredictor () reg . load_config ( \"network.pkl\" ) and finally load the weights of the trained model: reg . load ( load_path = 'best_model' )","title":"Loading a pretrained network"}]}